Speaker 1: Okay. Wonderful. It's my great pleasure and honor today to introduce Tai Lin, who is a philosophy professor at the University of Utah. I met him when he gave a fabulous talk and sort of seminar at, I guess, we'd call it Dizzy. We never quite agreed on the pronunciation of that institute, but the Diverse Intelligence is Summer Institute where Ty talked about value capture among other sort of concepts in the sort of the the questioning about, like, cognitive intelligence, social intelligence. And he is I'm also a big fan of his work on sort of games, which is a really fabulous paper, slash yeah. That I recommend you check out Otto's website exploring sort of the role of game design as a sort of medium for thinking about, I guess, the philosophical questions about agency. And, yeah. And he's joining us today to talk about value capture, which I'm sure he can do much better at explaining than I can. So let me in welcoming Tony.

Speaker 2: Hi, Dan. Hi, everyone. I'm Ti Nguyen. I am an associate professor at

Speaker 3: the University of Utah. I have

Speaker 2: to apologize doubly. First, yesterday was my first day of in person classes, and I just had five hours of lecturing through a mask to a large auditorium. So my voice is fucking blown. Like, I did not realize I had to, like, triple my volume to get through that mask. And I was like, I don't need a lapel mic. And so, like, the second is that I think in the post adrenaline boost, I managed to drop my laptop and break the monitor. So I'm currently coming at you from our eight year old family dying laptop that does not have any of my PowerPoint slides or notes. I think I'm just gonna wing it because everything is a disaster. So one of the interests, one of the places I got to where I'm going is, I work in two areas. One is social epistemology, which is the study of how social conditions, community conditions, and technology conditions change how we know and how we work on shared knowledge. And the other part of my work is in the philosophy of art. And I'm specifically interested in games. And as Josh mentioned, one of, I have this book, it's around some more, Games Agency as Art. And the idea of that is that games are an art form that work in the medium of agency, and that game designers specifically shape an alternate agency by specifying points, which give you a motivation or a goal. So I got really interested in the way that designers can specify a goal by specifying points, and we just glom onto it. Right? We have this ability to just take on points and aim ourselves at them. Most of the book is a love owed to games, except the last chapter, which is basically my account of why if you know, if you understand why games are great, you should be terrified about gamification. In particular, I think games are really good insofar as we get to step in and out of alternate kinds of agency and explore different ways of valuing. But we have some autonomous control over that. And I'm really worried that gamifications of work and education don't give us that. Instead, we have external value systems that are inflicted on us that we can't really escape, that are pervasive, that we can't move in and out of. So I've been working on that, and there are two projects that I think might interest you. So I'm just gonna briefly sketch these two different papers, one of which is out and one of which isn't out. And then we can talk about any of the details in Q and A, because I think I'm only supposed to talk for a few minutes. So the first paper is called Value Capture. And the second paper, which just came out, it actually, Josh, it came out between last time we met and this time, is called Transparency is Surveillance, a title which I'm told gave one of my friends in information science nightmares. And both of them are studies about how simplified metrics change our relationships to our values and goals. So let me just quickly run you through the general details. This I mean, I'm not I'll just give you a broad overview, and then you can ask me questions about any part of this. So value capture is a phenomenon. I actually talk about this at the end of the book. I'm defining value capture roughly in the following ways. One, your natural values are subtle, rich, and maybe a little inchoate. Two, you get placed in a setting, a social setting, often an institutional setting, that surrounds you with a simplified, often quantified version of that value. Three, the simplified version works its way into your motivation. And four, you become motivated not by your original subtle, rich, or natural value, but by the clear, crisp, quantified value. So some examples of this are okay. Twitter. I I I've if you're interested in this, this is a paper I've written called the gamification of Twitter, which is how game sorry. How Twitter gamifies communication, which is just about this. For the academics in here, citation rates. For journalists, clicks. For people that exercise, Fitbit. Right? In all of these cases, you might So here's one example. You might go into philosophy graduate school out of, I don't know, a love of wisdom, and then come out So I'm not sure most of you aren't in philosophy, but in philosophy, there's a This one dude, Brian Leiter, started issuing a ranked list of what he thought were the status of the top 20 journals. And now so many people in philosophy are oriented towards getting into the highest status list, right? Or you might go into journalism really interested in truth and connection, and come out really interested in your click numbers or your view numbers. Or you might go on Twitter interested in connection, understanding, empathy, and come out obsessed with your follower accounts, your tweet, retweets, your likes. So each of these is a case of value capture. So I'm worried about value capture in a lot of ways. One, a really, really interesting study of something that I consider a form of value capture is a really good book by two sociologists, Wendy Esblend and Michael Sauter. The book is called Engines of Anxiety, and it's a study of what happened to the American law school culture and legal educational culture when The US News and World Report starts issuing its law school rankings. And what they say this is the result of a ten year, very careful, empirically documented study. What they say is it changes everything. So before The US News and World Report started ranking law schools, there were there were no numerical rankings. Right? There were basically qualitative descriptions of what the schools were good at. And one of the things they note is before their rankings, law students, prospective law students, would try to figure out what school they wanted to go to by talking with each other about what they cared about about their legal education. And one of the things that happens after The US News and World Report is it seems like students no longer do this. So it's funny, like they have access to a lot of this stuff because a lot of this stuff is on internet forums, and they can actually track how law school, law student, perfective law student conversations change. And what they say is that law students no longer deliberate about the different values in different law schools' missions and strengths. And so no longer deliberate about, for the most part, about 99% of students, no longer deliberate about what they care about in their own legal education. They simply take their goal to be to go to the best law school where best is set by The US News and World Report. So to rephrase this in philosophical terms, what I wanna say is that there's a process of value self determination, which we should be engaging in, in which we figure out what we want our actual values to be, and value capture short circuits that. And in value capture, we outsource our value determination to an external source where that external source often has has motivations and interests that are quite skewed to our own. Right? So that's that's the key idea, that value capture outsources value deliberation. And the interests that guide the values as they're set in external and institutional sources are often interests that are guided by easy manageability, easy aggregability, and easy data management across a large scale institution. So in the paper, How Twitter Game Advise Communication, I take a look at how the life metric changes. So a lot of people seem to go I was like this. You go on Twitter, you wanna connect, and you get captured by getting more likes, right? So one response a lot of people have is, well, getting more likes is actually a fine thing to be directed by because it's just a proxy for successful communication. Right? It's just a quick representation. So my worry is the input mechanics by which we generate and aggregate likes loses a lot of information. So here's the kind of information you might lose. So the general idea is that likes oriented us towards quick popularity rather than long burn understanding. So here are a few things. One, on Twitter, when you click like, you typically click like the moment you see the tweet. So if a tweet you immediately like, you're like, Oh, that resonates with me. You hit like. If a tweet you're like, I don't know about that, and then it sticks in your craw, and and you think about it for two weeks, and then it changes your mind a month down the line, you're very unlikely to hit likes simply because of the nature of the interface. Right? So Twitter captures short term information. Here's another thing. This is, I think, this is the thing I care about the most. Twitter removes the amount of data removes information from the system about large scale complex nuanced degrees of valuing, and it reduces to a simple binary, whether you either like something or you don't. So there's a really interesting discussion here that I'm drawing on from a philosopher of art named Matt Stroll. And Matt Stroll has the following criticism of So you know Rotten Tomatoes, the aggregator for movie reviews? He notes that Rotten Tomatoes removes any information about how intense or overwhelming or like how much someone loves a movie, and it instead registers only whether something is slight, is on the positive side or on the negative side, and then it aggregates positive or negative. So here's something he notes. If you look at the history, like what most people think are great movies, when they come out, their reviews are quite divisive. Some people are like, Oh my God, this is amazing. This is the best thing ever. It moves me. And other people are like, Oh, this is fucking terrible. On the other hand, so notice that that kind of movie comes out as 50% on Rotten Tomatoes, which is a failure. Right? The kinds of movies that do 99% on Rotten Tomatoes are often movies in which all the critics are like, that's pretty good. That's a little above average. That's okay. Right? Does that make sense? So because the aggregation system removes all that information and highlights only the aggregation of above of being above average, then you lose a huge amount of information. You get So does that make sense? So if you're captured Here's the worry. If you're captured by the metrics in Twitter, then your values get steered in the direction, and your values get determined by all the kinds of shortcuts that get built into the Twitter aggregation mechanism. So I'm worried about that across the board, of course. I mean, I'm worried about this, about GPA. I'm worried about this, about citation rates. So that's value capture in general. The worry is that you are outsourcing your values. One final thought

Speaker 3: I don't know. I'll skip that.

Speaker 2: Let me just go to the other paper. So that's one paper, value capture. We can talk about that. The other paper, transparency surveillance, grew out of a footnote. It started as a one paragraph footnote in the last paper, and now it's its own 20,000 word paper that's just gotten published. And transparency surveillance, this may actually interest you more, is started, so I was reading, I was reading all this stuff about metrics for this project on value capture. And one thing you see over and over again is sociologists and anthropologists of quantification, like Sally Engelmery, saying things like, over and over again, we see cases in which progressives set up some system of transparency, centered around a metric. And in the early years, it's amazing, right? It reduces bias. It increases diversity. It exposes corruption. And then past the early years, it starts becoming this, like, all consuming quota to me. Right? So I'm on a lot of diversity and inclusion committees, and one of the things you find is in a lot of diversity and inclusion committees, all the complex values of diversity are gone. And what you haven't said is something like a university demanding that we increase the percentage of women and persons of color, and that's it. It's just that one number. So I got really interested in that. And I found this one bit of argument from a philosopher, Onara O'Neill, that I think not many people have talked about. And so she has this little bit in her lectures on trust where she says, People think that transparency and trust go together, but they actually come apart. Because in her version of the argument, If you know that you're gonna be constantly scrutinized, you're likely to hide your real reasons for doing things. So I try to spin this out into two arguments. By the way, I don't wanna say that transparency is bad and we should get rid of it. What I wanna say is that transparency is a heavy cost, and we should always consider the cost. So here are two arguments. One is the expertise argument. In most systems of transparency, experts are asked to explain their reasons to non experts. But experts' actual reasons are often expert. They're hard to explain to non experts. So in order for the experts to explain themselves, to oversight from non experts, experts have to either make up reasons, or if you're worried about the value capture stuff, experts may reorient their goals into terms that are comprehensible to nonexperts. So what this looks like is things like philosophy departments being in my period, philosophy departments being unable to justify their educational procedures in terms of intellectual virtue, curiosity, or reflection, but only able to justify them in terms that, say, a state legislature would expect, which is, say, employment rates and salary rates, right? That's a translation from an expert recognizable quality to an externally publicly accessible metric. Similarly, there's a great set of cases about how in congressional oversight over arts granting institutions, often the congressional congressional oversight says, Well, we think that you're giving away money to your friends. You need to prove that you're giving money to successful outfits, and the way that we can tell success is box office receipts. So if you're not funding things that make a lot of money, then what it seems like is you're being corrupt. So so sorry. I'm simplifying a really a lot of material. Let me let me try to get this together. So the general idea is that in so far as in transparency cases, experts are either incentivized are or are value captured by some metric, right, then their goals become centered on values that are set such they're explicable to non experts. In other words, the goals of expertise and the reasoning of experts becomes leashed to the understanding of nonexperts. Right? So but but here's the worry. We are in a world where

Speaker 4: the amount of knowledge

Speaker 2: is so vast that it can't fit in any human brain being's brain. Right? The whole point of science is that we need experts to distribute the cognitive division of labor, right, so that we can cope with the vastness of human knowledge. By the way, I just want to say so there's a standard worry about the kind of argument I'm giving that there's one class of experts and then a general class of, like, the non expert masses, and we're supposed to defer to the that's not what I'm talking about. What I'm interested in is the fact is that human knowledge is so vast that all of us may be experts in one thing but are non experts in everything else, right? Even if everybody in the world was a PhD, you would still be non experts in 99% of the things, So the aggregate of a world of PhDs is non expert in most things. So here's the general worry. The larger worry is that transparency here's one way to put the worry. Transparency asks us to take things out of the realm of the expert. So, by the way, any any all the cognitive science on expertise indicates that a lot of expert trained expertise shows up not as, like, explicable reasons, but as trained perceptivity, as sensitivity, as quickly noticing things, as being able to see something relevant as interesting, right, a kind of trained synthetic intuition. So here's the general work. The realm of the unstated, the non explicit, and the inchoate is definitely where corruption and bias live and transparency gets rid of that. The realm of the unstated and the intuitive is also where trained expertise and sensitivity live and transparency gets rid of that too. So the worry is that transparency is extremely rough medicine. It is often justified, but it has a very high cost. And one of my worries is that our culture in general is moving towards the more transparency the better, where I think the actual picture is something like there's a trade off between the demands of transparency, which are kind of surveilling oversight, and the need to trust experts outside of our understanding precisely because of the demands of the cognitive division of labour in a vastly huge realm of human knowledge. So basically, my view is we need to compromise between these two things, between trust and experts and transparency, rather than solving for all for trans all for transparency. Hopefully, you can see that these two are projects related. That's sorry. That's that's the best I can do on a borrowed laptop without my PowerPoints. I'm happy to take any of your questions. Sorry. That was chaotic.

Speaker 1: No. That's spectacular. So I think we have a bunch of comments in the chat. I have a couple of questions, but let me start with somebody else. Nick, do you wanna ask your first question?

Speaker 5: Yeah. Sure thing. Sorry for the background noise.

Speaker 3: I was just wondering

Speaker 5: if we have this dynamic where the quantifications are really no good, but but they represent they're trying to represent a rich and nuanced values. Do you know of any way that we can, iterate on the quantification so that we better approximate a rich and nuanced value over time?

Speaker 2: Okay. I'm about to go into deep waters of philosophy, and I might alienate all of you. So my worry is that there are kinds of, there's a kind of information that admits easily of quantification and a kind of information that does not admit easily of quantification. And the worry, this is something I'm working on right now, but the worries in general that a lot of our individual particular values are the kinds of things that don't admit of institutional quantification. Right? So let me emphasize it. It's not that I'm saying you couldn't, You can't It's not that you couldn't take your values and somehow attach some quantification to them. It's that the procedures by which we collect data in large scale institutions tends to, I think, is very good at collecting certain kinds of information and not good at collecting others. So one of the most useful background readings here is Theodore Porter's extraordinary book, Trust in Numbers. So I recommend this book to everyone. This kicks off this whole field of the sociology and anthropology of quantification. And this gets followed by Susan Starr and George Bowker's book, Sorting Things Out, which is an intellectual history of standards. So here's the general idea. Porter thinks it's this trade off between two kinds of knowledge, qualitative knowledge and quantitative knowledge. He thinks that each of them is good at something different, but he's worried that the world's going all quantitative. So here's the basic account. Qualitative knowledge is good at being nuanced, context sensitive, and information rich, but travels really badly between people of different contexts. Quantitative knowledge, as it's used in institutions, removes a lot of the information that's context sensitive and creates an invariant core, which can be understood and held steady across people of different contexts, and so easily inputted from different contexts and easily aggregated. So as a teacher, what I often think of is the difference here is between qualitative evaluations of student work and then the letter grade. Right? So the letter grade suppresses all kinds of information. Loses I mean, if I evaluate a student in philosophy, I could say all kinds of things to them, or to someone reading it that would be really meaningful about, like, their curiosity. None of this stuff is represented in the letter grade. Right? Because how do you aggregate that stuff? How does somebody, how does a Dean from the business school, you know, properly evaluate and understand some comment I have about, you know, Aristotelian intellectual curiosity, right? So the idea is that quantitative information is really good at tracking those qualities that are highly invariant across context and depend on low amounts of background knowledge to interpret. Qualitative knowledge is Oh, this is an amazing example of the Ted Bork. Sorry, I'm going on, but this is really interesting because this is an example of something halfway in between. It's a context rich piece of quantitative knowledge. But this will show the relationship to institutions. So Porter says that we used to measure in Europe, we, Europeans used to measure land in a unit called the hide. The hide is the amount of land required to feed an average family. So notice that the hide is incredibly information rich, but it's really variable in volume. Right? A hide by you know, fertile river is small, plains bigger, a hide in the desert is huge. Hides attract a really important functional quality, but they can be only administered from the local periphery where people have a large amount of knowledge of the intellectual environment. So when we shift to centralized governance, right, we have to shift from the high to the acre. The high is incredibly hard to aggregate. It's incredibly hard to macro control from a centralized institution. So I think one of the things this exposes is the worry isn't about quantification per se, it's about quantification as it's instantiated in large scale institutions. But that's also like, we need that shit, right? So here's a basic form of worry. Large scale institutions create half that need quantification that are not sensitive to local particulars. Insofar as we're value captured by those large scale metrics, then our values become insensitive to our local particulars. Right? I mean, what this looks like to me is a student not thinking about their education, not thinking about what makes them happy, but just trying to get the highest grade or getting the highest ranked law school. And I think this happens, like, fucking everywhere. Sorry. It's does that does that help with the question?

Speaker 5: It it helps me understand the context for why providing a solution to the question would be hard, but it doesn't necessarily give me a a framework or road map by which I can improve quantifications, which which you might be saying is just very difficult.

Speaker 2: Yeah. I mean, I there's there's just I remember having a conversation like this where someone was like I was like, Look, here's all the problems with Fitbit, because it's insensitive to different people's rich experience, richly different values about whatever. People aren't autonomous. And he was like, Well, let's just make an autonomy bit, and then it'll track what makes people free. And I'm like, what? You're just iterating the same problem. Like, all the problems Fitbit will now be there for the quality of, like, your valuing. So my actual worry here is that there are values that there are things that cannot be well tracked or targeted by large scale institutions. Wait. Wait. Let me give you a piece of background. So the book, one of the books I find the most inspiring here is James Scott Seeing Like a State. So this is an extraordinary book. The main idea of Seeing Like a State is that there are certain kinds of qualities that states can track easily because they can be easily put in the language of large scale quantification and certain qualities that they can't. And his view is basically that states, A, can't really register anything or C, anything that isn't puttable in kind of large scale quantification, and B, that states will wanna remake the world to eliminate parts of the world that they can't see, and remake the world to B, the kind of thing they can see. And when I hear your question, what I hear is, I don't know, someone who is enmeshed in a large scale state asking, what's the state level solution for this? And I mean, I actually think for many of these cases, I mean, maybe there's policy that, like, I think there are qualities that you can't get at through iterated large scale institutional quantification. I mean, my large scale view is that there are things that states are incredibly good at that we need states to do, that we need to do as collective endeavours, And there are other things that are better done at local or individual levels. And I'm not sure that you can target those through state level, endeavors. And that's, to some people, really freaky.

Speaker 3: I mean, I by the way, I

Speaker 2: think the things that step into this space are in the realm of art, aesthetics, and play. And a lot of that stuff is very hard to govern and very hard to institute through policy. So

Speaker 1: Yeah. Just actually just really quick follow-up to sort of next question. Such a little twist on this connection between transparency and metrics that you're articulating. One of the speakers, I think, like, last month, Dylan had Phil Manel, who's big recently at Berkeley and now is joining MIT. One of the topics he talked about was this idea of building so Dylan works in kind of AI scalability. Right? And he's also sort of really interesting questions about value, especially value alignment in AI, right, which kind of explores the question of, like, how you know, to what AI is really doing things that we actually want them to do, especially really complicated AI. And there he's very explicitly talking about, like, building better metrics to, make sure that AI is doing good things. But the way he kind of, like, talks about it, like the strategy he proposes that, okay, you can only assert these metrics. You you design these metrics as experts and you assert them via authority. So there are these kind of, like, magical metrics that are supposed to sort of guide the AI. And the only way they can sort of really implement them is by aggregating enough, you know, academic or moral or whatever authority or sort of like sort of imposing them or spreading them out, into the world. And I was wondering so, like, it seems like in this case, metrics only exist because of, you know, this, like, at the absence of transparency and such. Like, people just, like, you know, you have to say you trust them. You trust us, which kinda just, like that is for me the question. Like, maybe, like, you have some additional thoughts on this. It's just like, well, where do metrics really come from? Who's usually coming up with them? And it seems it's not usually, like it's usually a bunch of experts kind of in a non transparent way coming up with metrics to make things transparent.

Speaker 2: The best study of this I know is this really exceptional book from Sally Engel Mary called The Seductions of Quantification. So Sally Engel Mary is an anthropologist who does human rights work. And she ended up on the UN committees that made the various, like, metric trackers of, metrified trackers of how well different companies, or sorry, countries are doing in supporting human rights. And this is a book where she turned her anthropologist's eye on the committees she was sitting on. And the view is generally that what these committees are tasked with is taking these enormous amounts of complex data that has plural interpretations and represents an enormous amount of values, and then shrinking them down. To a and she says, okay, the process is she actually defines an indicator as any process that obviously, if we can get her definition right. That an indicator is a simplified report of an evaluation that hides the complexity of its manufacturing. Like that is our account of an indicator. And in general, people in the space, the anthropologists and the sociologists, think that the more you hide the complexity of the manufacturer, the more authoritative the metric comes. You need a kind of like immaculate conception of metrics. If people actually knew how fucked up and political and subject and like, I mean, she has all these cases where she's like, Oh yeah, and this would be more accurate. But then these people were like, Well, we're gonna walk out of the UN if you do this, because it's gonna make our country do bad. And they'd be like, Okay, okay, no, we'll change that. And it's like, It is fucked up, right? And then you get these metrics. So one account is that the Mary account emphasizes the political the politically charged nature and the concealing nature of metrics. I have another worry that even if you fix that stuff, even if metrics were made for the best of the and and that's just free of political bias, you'd still have the same problem, and the problem here looks like this. For me, it's an information pipeline problem. Here's the basic problem. Metrics as involved in transparency are designed for public oversight. The public has neither expertise nor considerable time. So a metric needs to compress a vast amount of information into a quick and legible form that someone can and that will always involve information loss. Right? I mean, the basic word and this is I feel I feel like a lot of what he's saying is anti democratic, and I I would like to have as much of a democratic impulse as possible. But the problem is I mean, in some sense, the more you put the public the aggregated public in charge via compressed metrics, the more you're asking the aggregate public to function as a CEO or president of every single thing out there at once simultaneously. And that's that's I mean, that is that is a bounded cognition problem. I mean, this is philosophy. Basically, there's a trade off. The whole problem is that we are individually that none of us have enough time to track all the information we need. And the more you ask for us, the less time and resources we can put in. And one way to crystallize the way about the standard setup of transparency is it's asking the public in general to devote all of the resources to managing everything at once, and quickly deciding for itself whether a metric is good. So in the paper, one of the things I'm really worried about is the way in which a lot of metrics seem good to the public simply because the public doesn't know enough to realize what shitty metrics they are. Right? There are tons of examples of this. One of my favourites is, for a long time, Charity Navigator ran with a metric called throughput. So this is a charity oversight foundation, and throughput is how much of donor money goes through. So they had a rank list if it has the most throughput. Turns out, of course, this eliminates waste at first, but after most places eliminate waste, to rise in the rankings, you have to cut internal costs, which means firing internal employees and not spending money on internal experts. This turns out to be a fucked up metric. I used this metric for six years because it looked good, because I didn't know anything about the charity world. And see, now here's one of my favourite examples in this space. So there's all these scientific papers discrediting wine experts. And the test they have is that, oh, wine experts blindfolded can't tell red from white wine. So it turns out that's premised on the idea that red wine and white wine are clearly different in flavor. But if you actually know a lot about wine, that's totally false, right? There are plenty of red wines that are thin and crisp, and plenty of white wines that are thick and oaky. And the idea that red wine versus white wine, being able to tell the difference in a blinded taste test, would be a good litmus test, is itself based on profound inner expertise about the world of wine. So there are just all these cases where what it looks like is a rushed public takes as the central goal setter for an institution under its power, a metric which is bad, but the public is you an expert to know that the metric is bad. Of course, you also can't let institutions run without oversight because we know there's a billion fucking problems that come up with that. So again, my claim isn't get rid of oversight. Trust the experts because we know corruption, bias, mission drift, all that stuff mounts in. It's that our oversight method is not cost free. Our oversight method is extremely costly. By the way, let me say one more quick thing because you were talking about machine learning. I got to sit in on Because I'm a philosopher that works on this stuff, I got invited to a conference from a bunch of machine learning experts working on artistry and expertise Sorry, art, artistic creativity. And it turned out that what a lot of them were doing were building machine learning networks. So you know that a lot of Netflix programs, Netflix shows, the scripts are partially algorithmically generated, right? A machine learning algorithm takes in data and says like, oh, it should have this theme, this mood, these kind of plot points. This is what our viewers will like. Yeah, this is already going on. And people are trying to build better ones. So I was in this thinking, people are like, oh, so we're making these machines that make better art, blah, blah, better art. And at some point, I raised my head. I was like, I'm sorry. I'm the one fucking philosopher in the room, but what's your measure of good art? And they were like, oh, yeah. We just harvest Netflix's data for what creates more engagement hours. And I'm like, that's not goodness of art. That's addictiveness. Right? Binge worthy of this is not the same as good. And I'm like they're like, well, show me another big dataset. I'm like, here here's the worry. The thing that makes art good I think this is an example for Nick. This thing that makes art good precisely may not be the kind of thing that's harvestable in mass datasets, where quick, mechanical, institutional friendly means. Right? I mean, what machine learning people keep telling me is we need a millions large dataset, right, to train our algorithms on, which means we need a way of getting data about what counts as good art or bad art that's harvestable quickly and cheaply and on a mass scale. And right so that that means that the kinds of measures you're drawn to are So one person was using the stars rating on Yelp. And, sorry, not Yelp, stockphoto.com. And another person was using, was using A lot of people are using engagement hours and how many episodes people watch in a row on Netflix. So again, I think maybe this will make things clear. Our values in art may be hyper personally tailored. They're not the kinds of things that you can harvest quantifications for on a mass scale. So this is not a comment on machine learning algorithm in theory. This is a comment about machine learning algorithms as they will be trained in the current environment by institutions given the kinds of datasets that are available to them.

Speaker 1: Or is that that's actually fabulous. And, also, it just kinda blows my mind that you are a philosopher of art. I'm we have not yet had that. This is an extremely interdisciplinary seminar, but we have not yet had that.

Speaker 2: Okay. Let me just say this is a rant I've had a few times. People think that these two disciplines are different. I think philosophy of art is a study of how humans communicate subtly. And it's a very so I've been doing a lot of work on Twitter. So I have this piece that's about to come out. And all the work in epistemology and communication theory didn't help me, but aesthetics work did because I was interested in what hyper short form, how the hyper short medium changed the way people communicate. Do you know who studies that? Philosophers think about haiku and sonnets and one liner jokes. That's the rich vein for thinking about what happens when communication has to happen in like a few syllables. So I mean, I just think it's the study. The philosophy of art is the study of how technology changes communication. Because it's the study of how different mediums change what gets expressed. So I think it's incredibly valuable. I mean, it's this huge vein about careful studies about how slightly different mediums change. I mean, my book is a study of how the medium of gain changes the kinds of experiences you can give people. It's not like a movie. The whole point is it's nothing like a movie. My claim is that games don't tell stories, they give you sculpted experience and practicality. And that is a different communication medium that's based in a different technology of recording. That that's why they go together.

Speaker 1: That makes a lot of sense. Okay. We actually have quite a queue here, so I'm gonna try to go through the Nathan, do you wanna

Speaker 6: sure. Yeah. Thank you so much for this. One thing I wanted to raise is something that came up in a conversation I've been having with Zargan, who's a member of this community, where I gave him what he regarded as what he responded was the highest compliment, which is maybe he was using cryptoeconomics to transcend itself. So I'm interested in this way in which quantified things might be able to actually escape quantification. And and this is important in the context of online governance because because quantification for any kind of computational process in some level is gonna be in the picture. And so the quest so so I've been really interested lately in studying ways in which, for instance, a project called the Orca Protocol is creating, like, these pods of users who pass some kind of quantifiable threshold, and then they become a kind of micro board of experts who have authority to make decisions over a certain domain in a community. Or, you know, another another case is something like, you know, liquid democracy where people are assigned or doing a kind of representative structure. I'm just wondering if if if you've explored ways in which in which you can enter into spaces of accountable dequantification, right, through a kind of quantified door.

Speaker 2: That's that's a really interesting question. First of all, I should say I don't really have a lot of solutions. I feel like I've been carefully working on describing this problem, and I'm hoping that people like you will actually come up with some practical solutions. One thing I can say is that I think one of the ways of putting the problem I'm talking about is it's not a liminal problem. It's an endemic problem to a situation where individuals who have individual values also need to work together in collectives. And that that's just like, we can ease the problem, but it's not gonna be something we can get rid of just because there's a basic tension here that has to do with how we process information at scale. So I don't think that we're just totally fucked because I mean, I think about this a lot as a teacher, because one of my views is like, I can't get rid of GPA, but I need to de emphasize it. And so one thing in general is that a lot of metrics start as people are like, Oh, this is just heuristic. This is just a rough proxy. This is just an approximate measure. And then it suddenly becomes central as everyone targets this. One of the things that Sally Engel Mary says in her book is every single time they publish this ranking of this human rights index, they're like, Oh, this is just a simplification. There's so much data. Here's this massive stack of data that you should really look at. And she just, no one looks at the massive stack of data. Everyone just looks at the rank. And then this is why the value capture stuff and the metric stuff go together. It seems to me that what happens is people get value captured by the most visible simplified metric. And so, like, if there were ways to use it I mean, we do need proxies, we do need data, we do need quick estimations. But if there was a way to block that from becoming to remember that it was just a proxy or just a heuristic, and keep it at arm's length and not have it become the overwhelming target. But the thing that I find, and the thing I'm really interested in, that by the way, the sociologists and the anthropologists report that it seems so inevitable in large scale institutions that once you create something that's supposed to be a a proxy, everyone just becomes obsessed with targeting it, especially once a generation passes. This is in the Esplanade Every book in this space is like, this just keeps happening. PS, psychologists haven't studied this as much. I don't know why. All the literature I'm looking at is sociological. That's one thing. The other thing is, and the suggestions you give seem right to me, because if I'm right, this problem particularly arises from the desire to aggregate information and information making and centralize it. And to make it possible I mean, this is why it's so, like The the problem really is about trying to get everyone to make every give all the information to everyone to make everyone to make every decision. And I think the like, that democratic ideal just runs against harsh cognitive limitation. That's why these the the thing you're describing is interestingly very close to a proposal that a philosopher, Alex Guerrero, made. He has this amazing paper called, The Lautocratic Alternative. And the paper's basically like, every attempt we've ever tried to run with democracy, as we see it, capital interests overwhelm the fact that money just goes So he's like, since we can't do better than this, since we can't actually get it right this way, the best alternative is just holding a fucking lottery for who's in charge. And he thinks that we've already, like this is already the reasoning between jury systems. His proposal is exactly this. He was like, what we need is lotteries to create small scale governance groups, And we let people occupy those governance groups for five years when they're lotteried in, and they get to become experts in a small But they're immune to capital interests. They're representative of the people through randomization, but they're not answerable. I mean, in my thought, they're not quite answerable via transparent metrics getting you this nonexpertise problem. I mean, that's, like, a wild philosophy. I don't I mean, that just No.

Speaker 6: I mean, cert certification mechanisms are very much in vogue right now in this in this community for precisely those reasons. You get deliberation, but you don't get captured. So, yeah.

Speaker 2: Thank you so much for

Speaker 6: that. Yeah.

Speaker 2: If you could tell me if you if you can send me stuff from your space about exactly that, like how assertion mechanisms avoid capture, that is the thing. I would love to see that information. I feel like tracking what's happening with metrics is happening in various disconnected communities. And like, it's so hard to track all this stuff. Let me respond to one quick thing I saw in the comments really fast about whether this is money, whether money counts as one of these megametrics. I wanna say two things. One, absolutely, money is the central metric. People are captured by it, obviously. One reason I don't want to emphasize it is because when I talk about this stuff, a lot of people want to immediately say, Oh, what you're saying is that capitalism ruins everything, and capitalism, like, the ethic of capitalism pervades. My view is actually that this is an information processing problem, and a problem of governance at scale, and that capitalism is one head of this problem, but centralized bureaucracies and socialism are another. So if you read Scott, Seeing Like a State, one of the things I find really compelling in Scott is he finds the same quantification scale aggregation problem, both in globalized capital markets and in centralized communist bureaucracies. And he identifies it as a problem of large scale institutionalization, of which capital markets are one and bureaucracies are another. And for me, I mean, this is the philosopher, not the person grounded in reality. What I'm really worried about is This should not be a forefront worry. But if we actually get to some utopian, non capitalist worry, a lot of this capitalism specific worries might drop away. But the problems that are endemic to bureaucracies and large scale institutionalization, we might expect to increase. And if I'm right, that this is a problem of information aggregation, and not specifically when capitalism is just one solution to the information aggregation problem, you should expect this shit to get worse. And it's funny to me that a lot of academics in my space, who are very Marxist, very leftist, think this stuff will disappear in a capitalism when they complain in the daily life about their non profits' bureaucratic oversight inflicting exactly this kind of problem. So I try to be careful about where the problem is sourced. This is really up in the clouds of, like, philosophy. Sorry.

Speaker 1: Actually, since you mentioned seeing, like I said, Seth, do you wanna take it away? It's an excellent segue.

Speaker 4: Sure. Sure. First, a comment. You've apologized a lot for not being able to provide solutions. But if your argument's basically that, hey, guys. Different choices have upsides and downsides, then you you don't really have to propose solutions because, you know, everything is upsides and downsides, and we should have those in mind when we design institutions. And but to get to my question, your kind your argument kind of assumes that quantification, like, numbers and language are different in kind. But if we allow a sort of continuum in, you know, representations of ideas, then then your argument, you know, is exactly the same. Your the cons we should have the same concern about language and talking and ideas, and and and the concern becomes the things that language can't capture and all the vibes I'm missing when I'm trying to come to an agreement with someone. You know, which is valid, but, also, I guess what I'm doing is a little bit of an ad absurdum argument against your argument, which is that, yeah, like, we don't have ESP. I don't have a perfect representation of your mental state. There's gonna be some some loss. Sure. Like, yeah. But so, like, can you get me deeper than that?

Speaker 2: You're describing the next the three papers I'm working on right now about extending this to language in general. So my claim isn't that the argument The claim isn't that the argument is only about quantification. I think quantification is the clearest example and maybe the most extreme example of a number of tendencies. But what's really driving it is standardization across large scale. And that happens with language too. So in the transparency paper, I spend most of the time talking about quantification, possibly because it has the most empirical weight. But I also spend a bunch of time talking about when you get the same problems arising from standardized justificatory language across large scale institutions. So

Speaker 4: Oh, why not all language? Like, why stop there? Why not all language?

Speaker 2: Right. Because At

Speaker 4: that point, you're just describing, like, hey, man. We should really vibe each other out more. And, like,

Speaker 2: with The the quest if you assume that all language okay. If you assume that no dialects, then the problem is one for all language. So so hold on. The problem is a problem that arises from the attempt to standardize an input and standardize a meaning across a large scale. So the so the word so I'm I'm trying to That's think about how I can explain this without, like so there's a paper I'm working on with the philosopher language. Basically, there's something like, you have exactly the same problem if you try to fix meanings across communities from a central source. So if you try to fix the meaning of a term and impose a meaning of a term from the top down or from the institutionalized source without letting different small scale local communities tailor language to their uses, then you get the same problem. But the problem isn't getting rid of language. The problem isn't the problem isn't like all or nothing. The problem is the degree of Okay. So the so the paper we're maybe it'll help. The paper we're writing is called semantic self determination, and the argument is that pieces of language the way you cut up the concept Language cuts up the conceptual landscape, and each cutting up of the conceptual landscape has a different pragmatic impact. And since they're different, and since different groups have different needs from their conceptual frameworks, it should be that conceptual frameworks are set often at a relatively local level. So the view we're calling semantic federalism, and the view is something like, no. There's some terms that we need stored up in common so we communicate, but a lot of terms should be driven

Speaker 4: on So so, in the Linux foundation okay. Maybe I should move along. But oh, yeah. Yeah. We're at the hour, aren't we? Oh, I'm

Speaker 1: sorry, Michael. Michael, do you wanna ask you a question? If I don't know if do you need to, like, jump immediately at the hour?

Speaker 3: I don't need to jump immediately at the hour. I think my question will take more than one minute, though. So, certainly, if we're gonna have an answer. So I'm happy to continue if if you can stay around to you.

Speaker 2: So Yep. I can stay around for a few minutes. Okay.

Speaker 3: Okay. So, I mean, did you wanna finish that?

Speaker 2: Well, the only thing I can say is it the the the the end point for me this is, like, this is unwritten as of yet. But the end point for me is really a worry about the standard that a lot of people have of public reason, that reasons are comprehensible to everyone. And my worry is that the standard of public reason depends on, a shared standard of justification and a communication a shared communication network. And the more we demand that that be fully universalized, the more we get these kinds of problems. So this the metric problem is just the tip of something that is about the demand for reasons that we can all share and comprehend universally. So yeah. I mean, but you can have language without having a single language for everybody. But now we're going into space that's like, that I need my philosopher of language compatriot to help fill out my brain.

Speaker 1: That's it. Anyone jump in? Sure.

Speaker 3: So I'm so my my my background is more in sociology, and and and I found the sort of, like, the discussion of, like I'm I'm curious to hear a little bit what you think about natural values. Right. I think the discussions of natural values and information loss through aggregation and the idea that we should be upset about that strikes me as a little bit strange. Yeah. The simple version is sort of the cartoon sociologist version, which is like where the values come from except from the ID category systems, metrics, so on and so as reflecting the metrics. The more detailed version is draws a little bit more from the sociological literature on categorization and on status. And so I'm thinking about this in part because of the examples of law schools and things like that, which suggest essentially the reason we rely on category systems or things like status is because they're better than the alternatives that we would be doing instead of it like, that we'd be using instead. Right? So the so you talked a little bit about some of these sort of, like, cognitive sort of sense in which we're, like, sort of having having to evaluate everything from first principles would be paralyzing. We wouldn't do anything. Right. So sociological literature on status suggests that people rely on things like stat like like like when deciding whether to go to a law school in part because established because they they do it in situations where it's difficult to evaluate underlying quality. So, like, it's actually really hard to tell if a law school would be a good match for you Right. Or whether a wine would be particularly good or whether a new assistant professor that you're gonna hire in your department is gonna be any good or whether, don't know, these sorts of things. And so I have no ability to evaluate whether jewelry is any good. And so we rely on status k. And these sort of rankings and metrics and these sorts of things in those situations. And some of that works builds on this sort of more classic anthropological work by, like, Right. You know, like Douglas on purity and taboo. Yeah. Where we divide the world into good and bad things because because we want the benefit of that information loss.

Speaker 4: Right.

Speaker 3: He's evaluating everything that comes across that we come across in terms of, you know, like, I don't know. Is this a good thing to eat? I don't know. So we just, like, put things and things and bad things, and we build these rule systems which are better than the alternatives. So that sort of connects to this cognitive argument that I've heard sort of come out in a few ways. I think that the James c Scott analogy is instructive because of how it's different. The difference in that situation to something like a law school ranking or is that I can still choose to go to a law school out somewhere else. Right? There's this there's a power and coercion when the state is sort of doing it and bending the world sort of like fit its world in a way that it's not here. So it's just like it's it's hard for me to get upset with Charity Navigator for embracing imperfect measures of throughput even though I've used that the their rankings as well because their bad measures are better than my whatever it is that I would do in absence of them. Right? Like, I'm, like, pretty confident about that. And so and so, like, I guess that I'm so there's sort of two questions sort of poorly baked into there. One is sort of, like, what constitutes natural value and how do we think about that? And then the second one is maybe just another version of the same question that everyone has asked you about, like, what do we do in some sense.

Speaker 2: Right. So let me the second question is so the The second question is a little bit easier, not in the fact that we should gesture at. One setup is immediate individuals pick the metric. Another is individuals find intermediary experts, regulators, something. And so the question becomes so for me, one of the interesting things one way I put it is, you know, I do epistemology, study of knowledge, and most people in epistemology think the main question is, how do you know what you know? And I think the main question is, who do you pick how to trust? I think in a lot of times, picking a metric that you think you understand is actually less effective than trying to figure out an expert that you think you can trust, and that that looks different. How that ends up looking is incredibly complicated. So that that that's the first thing. That's that's the second thing you were talking about. So the worry about natural values. So I I hyper simplified this stuff. The the the paper actually has a footnote that says, If I could talk about our natural real values, I could skip the next half of this paper, and we could just be like, you know, institutions prefer that. But I don't think that. What I think is that we are social beings that acquire values from social settings, right? Blah, blah. So

Speaker 3: the argument of

Speaker 2: the paper is something like, we are social beings. We acquire our values from social sources, but we also typically do another thing, which is we fine tune and adjust those values to our particular environment, our particular personality. So the thing that I'm worried about value capture by institutional metrics doing is cutting off that latter step of fine tuning. So the way this looks like so, Elijah Milgram is a philosopher, who has this lovely account of how he thinks what we he thinks that he has this book called Practical Induction, and he thinks that we get a lot of our values and we improve them experientially. And what that looks like is we learn a value. We're like, Oh, you're supposed to do this kind of thing. And you try it out, and you're like, That makes me kind of hard at that. And then you adjust what you aim at a little bit. On the small scale for me, this often looks like, I mean, I had a journey where I went from a very sedentary person to a fairly active person. And at first I did it by like trying to get mileage. And then I did it by like, you know. And as I like live life under a particular athletic goal or value for a while, I would be either happier, sadder, or more filled, and then I might adjust it a little bit. And so now like, you know, I chase beautiful rock climbing problems instead of like, So you can adjust your target given the feedback of your personality and how your life goes. So the worry is that value capture, by hooking you up to a standard that's not set by you, but set in common at an institutional scale, cuts off that latter individualization step. One way I put it is that my worry is that what value capture is, is Scott style legibleization extended to our internal values. What it's doing is the state has lots of reasons to get us to, instead of individualizing our values to a particular personality and emotion, to attach our values to something that is state measurable and state targetable. So it's the non adjusted. You can differentiate between the social origin point and the openness and flex to individual or small scale adjustability of a statement of a value or a goal. This is leading In the background here, there's a enormously complicated, probably too technical paper I'm working on with a metaphysician about the difference between the metaphysics of things that flux dynamically and things that are tied down by the need to write them in some standard. And the worry is that there's a class of things that are better when they flux dynamically, and they could be passed from person to person in a fluxy dynamic way. But that certain, recording standards change their fluxiness. And that that that's one way to put the problem.

Speaker 1: I I really don't wanna sort of stop this here, but this is a really fabulous discussion. But we've actually got to end this call, so we can start end of the call in the same channel. Could I ask everyone in the spirit of our sort of regular tradition to unmute themselves? And in the next three seconds, three, two, one. Thanks, Steve.

Speaker 2: Thank you so much.

Speaker 1: Alright. Everybody. Spectacular.

Speaker 2: Bye.

Speaker 1: See you all soon. And join the discussions in the Medigob seminar. Slack.