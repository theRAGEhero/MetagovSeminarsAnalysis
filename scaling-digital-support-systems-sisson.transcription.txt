Speaker 1: There we go. Hello, and welcome to this particular seminar. This seminar is exciting because it's a combination between DeSci Labs, DeSci Foundation, the Future of Science seminar, and MediGov. So hopefully, we'll have a little bit of a mix of different communities here. The main premise of this collaboration was the intersection between research and understanding how we're doing research well, which is what DSAT Foundation works on, and then governance, how are we in control of the way we're doing research well. Well? And so as well as the final thing is everybody keeps talking about AI, and how are we gonna actually use that in a functional way? So over the next four months, we're gonna have four different presentations that are somehow related to this. This first one is with David, which I'm super excited about focused on, like, data and how we're actually formatting it and building it and using it in a way that is gonna be useful for all of our fun new machines. And then next one that we have lined up is based on ethics and how do we use AI in a way that's in line with planetary and human boundaries. So I'm excited for that. For those who are new to Medigov, I'll give a quick intro. Medigov is a lovely skunkworks of experimentation around governance and focused specifically on digital governance. So it's focused on how are we collaborating to build tools that help us govern ourselves better. And there's a couple of different lines to that. One is actually focusing on supporting different projects or bringing projects together who are building tools. And another one is kind of this community building thing of education and getting people together. The second org that's involved in this collaboration is DeSci Foundation. The DeSci Foundation is focused heavily on how are we doing research better and specifically how are we taking these metascience insights and feeding them into the new tools that are being built so that the tools aren't just unbased and in the real work that needs to happen. And in all of this lovely soup of fun people doing fun things, I'm super excited to invite David Sasson as the very first speaker to talk to us today. I've gotten to work with David at the Koipon project hosted here in MediGov over the past year and a half, some amount of time. And I think something I found very valuable about talking with David is, he feels it feels like he always brings in, like, what's real, like, TM, when it comes to, like, people talking about AI and random new things. It's like, okay. Well, like, let's base this in reality. So I am super excited to hear from him today and get a little bit more of a structured presentation around, yeah, how we're swimming in a world of data and how that's gonna help us. And with that, I will pass it over to David.

Speaker 2: Thank you very much. So we get things set up. Oops. There. Now, hopefully, everybody's seeing the slide or my slide deck.

Speaker 1: Yeah. We can start

Speaker 2: with the title. Scaling digital decision support systems. So today, I'm gonna talk about how to drink from a fire hose and not drown. Hopefully, some useful stuff will come out of it. Right? I'm going to use an ontology developed in a paper by Alain Demir and me, called Why is There Data? For the domain of data driven decision support systems. It's a functional derivative that's, of the, sort of traditionally more structured data to information, to knowledge, to wisdom hierarchy. And in the next couple of slides, I'm going to briefly review this ontology using this particular schematic representation. And if you're interested in more details, the QR code on this slide is a link to that paper. So, as shown in the schematic, we've broken down a data driven support system into three subsystems. Three subsystems are necessary we found because data driven decisions are for, data driven decision support, because there are three quite different processing stages required, to go from observed data to decided action. There's an abstraction phase. There's a generalization phase, and there's a reification phase. And I need to point out, two features of the system that show up more or less as stubs in this schematic. They're stubbed because it improves the, overall legibility of the diagram, but that doesn't indicate the importance of them to this system. First of all, there's there's feedback between subsystems. And the feedback here is dubbed in as the system egress, ports from, each of the subsystems outputs. That's the little dots that head outside of the big black box or the big, the heavy black outline box. And it's also I should draw attention to the fact that there is feedback between reification and generalization stage is the last two is explicit if a little simple in this diagram. Basically, because there's a need to draw attention to the, the, buffering that, the reification stage benefits from effectively the initial abstraction phase. And second, there's a somewhat fractal nature to the system, which I don't have the artistic ability to diagram. There's a self similarity across scales that's implied by the color coding of operations within each subsystem. And also the more complex sort of acquisition like, hence pink operations labeled collect or assemble, can themselves be considered a sort of a scaled down version of this three part system as a whole. So, you know, this whole thing can operate as as as in a in a in a fractal fashion. So with that, I wanna briefly focus on each of these subsystems. You know, start out with in the ontology of why is there data? The abstraction stage is called information processing, which may be more familiar with. Its purpose is to find the figure in the ground using the vocabulary of gestalt psychology or the signal and the noise using the vocabulary of electrical engineering from specific sets of observations. And this purpose can be summarized in the word in as a word as as sorting. And this process is shown here as being basically linear, even if you take into account the implied feedback that I mentioned earlier. And lastly, if you if we look at this whole process and and and and its dependency on active processing to really work, with active processing, we end up using this with things that look like what we call data lakes. And without active processing, we end up with stuff that's usually termed a data swamp. And so from there, we go to the next stage. The ontology names the generalization stage knowledge processing, and its purpose is to integrate the figures and the signals coming from the information processing into a coherent world model and then use that world model to assess the past and predict the future. And this purpose can be summarized in the phrase as sense making. The process is shown here as being iterative to the extent that feedback is an essential part of this subsystem. And, again, if we it it requires active processing. And with it, we get things that look like, say, laboratories or libraries or museums. And without it, we end up with stuff that looks more like an archive or even a tomb. And then finally, this ontology calls the reification stage understanding, and its purpose is to extrapolate an action plan from the assessments and predictions of the knowledge processing that will hopefully produce some desired outcomes in a future that contains a novel ground or sick or noise and may even have some new figures or signals in it that the system has never seen before. And I'll refer to this process as decision making. And this process is shown here as being circular, largely because it's been modeled after, processes of deliberation. And to that end, when I think of active processing in this system, I usually think of things like a court docket. So a schedule with an agenda and some some accepted documentation that follows it around. And without a active processing, it's more or less just a reporter, you know, what's been done filed away in some volume somewhere. So putting the system back together again, I'd like to point out that the flow of data through a data decision support system of a goblet manufacturer is quite different to that from a communications company and different still from a bar serving liquid libations to disinhibit conversation. Although the same schematic can solve satisfy all three use cases. And even in this sort of black and white context depicted below here, there's a degree of ambiguity. The goblet system must consider the faces just as the faces system must consider the goblet. Even if the faces consider the goblet to be a nuisance variable, then the goblet considers the faces similarly. With such systems, when presented with novel goblet, the faces can predict their profiles. And with when presented with novel faces, the goblet can predict its shape. And then with more complex as the comp as the context grows more complex, there's more ambiguity to sort through, make sense of, and come to understandings leading to data driven decisions. And as complexity increases, so do the volume and variety and velocity of the data flowing through the system. Accordingly, each of the three subsystems must scale in keeping with its purpose and while maintaining overall system coherence and fidelity. And I'll use the term impedance matching for this. Even though impedance matching has quite specific meanings in electrical engineering, I think it it it's a good metaphor for this maintaining overall system coherence and fidelity in keeping the system purpose. So people have been making decisions for a long time. Hence, the support the decision support systems, including the data driven variety are kind of old. Through most of the twentieth century, scaling of data driven, decision support systems was a manual process. That is to say making bigger system, bigger systems involve more people. Such systems can operate like well oiled machines because of good impedance matching at all of the junctions. Largely due to the fact that all of the components having, have compatible properties in the frequency domain where, impedance matching is is is defined. And the I can say that the industrial revolution was made possible by advances in operating machinery, but the operators were by and large people, and people are after all people. But all this time, digitization has been creeping in. And here we see the beginning of the big data era, drinking from the firehose. The fire hose of digitally sourced observation data was initially directed at decision support systems that were not designed to handle the streams for volume, variety, and velocity with the result kinda shown here. But by the end of 02/2015, say, the end of, the big data era, we had reworked the first stage so that we could scale information processing to support arbitrarily large volume, variety, and velocity. Just took money. But what about the rest of the system? All we've done here really is to move the bottleneck. In 02/2015, there was no effective mechanized alternative to human brains for knowledge processing stage, which is largely symbolic computation in the form of reading, writing, and arithmetic. And this sort of computation happens to be a strong suit of human brains. Now we learned the trick of manually scaling knowledge processing back in the twentieth century when we had to take but we had to take it to a new level here to keep up with the automated information processing. So we've seen in the years leading up to 2020, massive hiring of analysts and middle managers to fill all of these new latent space desks to manage routing and to manage the routing of information between them. It's of note that this trend was particularly strong across at least six of the so called magnificent seven, Apple, Microsoft, Alphabet, Amazon, Meta, Nvidia, and Tesla. More interesting maybe is that one of those the one not seen with this trend was NVIDIA. However, the its omission is is more for a lack of data than any positive reason. But still, for reasons that I hope will become apparent in this talk, I think keeping an eye on on on NVIDIA might still be interesting and offer comparisons to the growth of some of the other magnificent seven companies. And I just wanna make full disclosure here that my research assistant for this presentation was Jack GPT five. So you can take this trend with a grain of salt. Still, it aligns with my experience managing development and analytic teams for an ad tech company during the same time. This is an example of how, different stages of this system must scale differently. The scaling of information processing of big data was achieved through homogeneous horizontal scaling. That is the task is divided. The task of of of drinking from the fire hose is divided across many identical units, each equipped with its own straw. Knowledge processing, however, is not homogeneous, so horizontal scaling must embrace heterogeneity. Keeping in mind that even though the ranks in of desks shown here, look homogeneous, each desk at each desk is occupied by an individual human, each with their own capabilities. The effect is many different adaptive units interconnected in adaptive ways processing reduced by information processing standards, but still substantial stream of knowledge objects. Now assuming that a massive build out of latent space in the form of middle management and analytics staff has worked to scale knowledge processing to absorb the volume, variety, and velocity of big data information processing. And then, and my rough exploration with the help of ChatGPT of Magnificent Seven growth and valuation and staffing trends from 2010 to 2020 suggest that this is at least worth looking into. And hopefully, people actually have, and and and re results can be cited. So that happens what happens to decision making? Well, not much. It's still pretty much black and white. Perhaps it always will be. It is, after all, kind of a binary sort of process. But as decision making becomes the bottleneck, decisions are made increasing by using increasingly simple models. The simplest being just telling what it's gonna cost. So we get to where we are today. Massive companies, Amazon, for instance, has about one and a half million employees, generating massive revenues required to afford the massive latent spaces of people, analysts and middle managers, etcetera, required to construct world models that adequately absorb the massive feeds provided by their fully automated information processing systems to generate high fidelity, high dimensional predictions, which feed decision makers constrained by real time concerns and who are largely who rely on largely unscaled human computational capacity and a top down organizational architecture designed, for a time when computation was people all the way down. To be of use to these decision makers, those high fidelity, high dimensional predictions get compressed into a small number of dimensions, like one or two or three. One of which is always monetary. The monetary predictions happen to be really, really good, which enables these companies to outcompete most of their most of their, fellow, fellow competitors on a strictly financial basis. But in the process, a lot of externalities are left begging on the table. Well, now it's 02/2025. We can rebuild this. We have the technologies. We have the capability to make the world's first bionic. Yeah. We've heard that before. But I do wanna make a couple of predictions or a couple of hypotheses to propose, here as questions. Aspirationally, do we have to assume that bigger is always better and that the aggregate of enough individual gain is equivalent to common good? And more assertively, can digital symbolic computation in conjunction with human symbolic computation reduce the costs of standing up a large enough latent space to hold a world model capable of absorbing big data information processing feeds and producing competitively accurate predictions to the point of being affordable to a company with a payroll in the hundreds? And can a few thousands so equipped companies find ways to successfully compete with today's giants based on the aggregate result of their many more focused predictions, leaving fewer externalities, ethics, for instance, betting on the table. Now accepting that the $6,000,000 man was a TV show. And while being an accurate indicator of inflation, it isn't going to provide much insight into how to build organizations in the future. Where does that lead us? And before moving on, I wanna make a summary of sort of where we're at. So first, we're just I'm describing a dynamical system here, not a static one. So temporal concerns are important. And as such, we can look at the at the three systems. There's initial one that performs sorting, and it has to do this in real time. That's clock time or what some like to call Kronos time. And it involves a real linear flow that produces immutable results and uses essentially stateless processes to do that. And all of the above lead to something that's really machine scalable. This is in machine's wheelhouse. As we've seen, we've done this. This is this is mechanized. The next one, uses, is is is for sense making. Now it's based on it uses sort of an internal measurement of time or Heidegger's concept of eigenzite. It uses an iterative flow that produces evolving results with very stateful processes, and it's gonna require some sort of hybrid scaling that I'll at least gesture at as we go I go further into this talk. And then lastly, this all feeds into a decision making, as I said, somewhat binary binary end end result. It is like the beginning real time, but here we're talking more like Kairos time or the appropriate time. And it uses a very circular flow. It could be considered linear. Just take take the information's processing system and feed its beginning its end into its beginning. It produces expedient results. It has to make a decision that in a in a time frame where the decision is useful, that is Cairo's time. And its processes, again, are essentially stateless. We we feed data into them, but they remain the same throughout. And as I've said, this proceed this part this part of the process may not be scalable in the sense that we've talked about in the other ones that we can just keep making it bigger. Let me tie into that. So to move on, I think it would be in in useful to shift perspectives from mimicry to augmentation. Mimicry and augmentation to integration and control. That is we're gonna go from bionics to cybernetics. And to do this, we must respect the differences between humans and machines just as we must respect the differences between the three stages of decision making. And I've mentioned, to some, here that, I have a background in advertising. So the following could also be considered my product placement part of this presentation. So will generative AI solve the knowledge processing bottleneck? No. As Zargam and Ben Merer point out in their in their recently published article, with generative AI, the bottleneck is no longer content generation. It's judgment regarding the content. Gen AI is sufficiently good at what it does, generating believable artifacts with that without a controller, it won't be the paperclip apocalypse that buries civilization. It will be the paper apocalypse. And if discernment is the process by which a decision is reached, there's a basic human machine difference. Machine's discernment involves acuity, that is the ability to detect differences. Human discernment involves judgment, that is the ability to come to a sensible conclusion. And in judgment, I think there's room for ethics, But for acuity, not so much. So, again, we wanna make sure this stays a hybrid at the most system. With possibly humans being the only, available of ethical, controllers for these processes. So let's assume we want sustainability of of a reactor as as opposed to the impulsivity of a warhead as argument, but near put it. And that can key control is the key difference between a reactor and a warhead, and that human judgment is the key to ethical control. How do we get there? Then we can look at recent published papers. Renee et al point out that the way forward involves integrating, not replacing human judgment with machine acuity. Knowledge processing can, can be engineered to support Renee at all's concept of artificial organizational intelligence. And this paper describes their work moving from concept to practice with the telescope project that uses COI protocols to achieve just such integrations. And I invite you all to look into what what, they present. And then as far as, decision making, the last stage, Schneider et al provide a review of research into attention economics, governance surfaces, and how they're interrelated. And this review is followed by a series of case studies that are used to develop heuristics for retention in governance. The understanding part of decision support ontology that I've used here is largely unchanged from its first presentation in as computer augmented governance back about three years ago. And I'll admit that knowledge processing is the part, the knowledge processing part has been my primary concern of late. However, I think that the understanding part deserves some attention, especially with regards to the heuristics presented here in this paper. And for me, that's where I'm going to be spending, the next few weeks, maybe months. And in closing, I'd like to say that this effort has left me pretty much astonished by the productivity that the communities that MediGov and DeSci foster. And given my corporate background, I find the juxtaposition of the lack of compare apparent organization with the apparent effectiveness of these organizations nicely ironic. And, thank you for letting me participate. Any questions?

Speaker 1: Thank you so much, David. There's already a bunch of questions coming in through chat. I see the first one is coming from Anne, but Anne, I might actually drop into the the question after yours because it's a clarification question so that we can all get on the same page before jumping into some of the other questions. So I'm going to go to you, Chris, you had mentioned that you're unfamiliar with the distinctions of time and you wanted a clarification there. Do you want to give any flavor to that?

Speaker 3: Sure. Sounded like it's very, it thought it was important. And so so that was partly what made me very interested in in in understanding this distinction. So I did wow. Interesting that the divide of time and time in different ways. And if you thought it's important, then I I thought it would be interesting to understand why you think so.

Speaker 2: There's there's a member of the MediGov community case, who can who can wax on this as as long as you want. I'll try to be brief. So Kronos time is wall clock time. It it's sort of the the steady progression of of time. Heidegger had this idea of internal time or he he used the word He was German, and German is a nice language to invent words because you can cram downs together. And internal time is simply the time it takes, you know, for for some internal process to work. And it's it's used a lot in in in computer systems as as non real time. It's everything but real time. It's just the time it takes to do some processing. And then finally, Kairos time, we're we're again we're back we're back into the world of real time. These are things that are happening in in the real world, but it's more of event. Looking at world the the the time is the passage of events. And optimally, these events happen at the opportune time. So think of kairos time as being the opportune time. So we've got clock time, internal time, and the opportune time.

Speaker 1: Cool. Thank you. I think the next then I'll go up to your question, Anne.

Speaker 4: Well, for context, this question comes from Ecology. And the setting there is that, you have a a set of interacting organisms, and you draw a box around those organisms and you label it something like weather. And then the sort of dynamic tension in ecological model modeling is about does how does the weather touch those organisms? Is there sort of some sort of global variable? Does it touch one organism more than another? Generating some kind of keystone effect, does it touch them all equally? And that's how come you drew the box around the whole thing. I don't know that there's any coherent system for incorporating all of those potentials into ecological modeling, but I do think it's a kind of question that applies to your data system as well. Where are the points of interaction into the context?

Speaker 2: The the the model itself is what I would concert to to context driven. The the the manifestation of the the way we've sort of developed out the concepts that I presented here are in a in a technology we we lovingly call call COI, for knowledge organizational infrastructure. And one of the main points of COI is it is a protocol that sort of specifies, first of all, a network architecture. First of all, what the what the nodes are, and also being a network, how they interconnect. And they use, for communication, a thing we call what we call our IDs or reference identifiers. And I like to I I try to stress whenever I can, and maybe repetitive to some people here, The reference IDs are don't have a universal or uniform definition. They're defined locally, and then a locale can define their own nomenclatures. And then using those if we wanna use those nomenclatures across domains, they have to negotiate what that means. And I think that that applies to the system that you you you mentioned in terms of of of ecology. That if we're gonna model systems, we have to we have to realize that the models aren't the system. That each of the models has to sort of stand and and look at a part of the system. Otherwise, why are we studying the system? Let's just observe it. The and in that state, I think it's important to have different models for the different parts and let those models talk to each other rather than try to come up with some overarching model that describes the whole thing. I I I it it as a as as sort of a rule of thumb, I I get I'm hesitant or I'm skeptical of anything that starts with a u for universal uniform.

Speaker 4: So would you say that the governance is both embedded in the structure of the data flow system and in the context, or is it primarily context?

Speaker 2: That's interesting. Thank you. I'm I'm not sure that in this system, there's a clean separation between context and the system. Sort of the the the the the particular context in which you're observing defines essentially what the system needs to do. So they're tightly coupled. Was that

Speaker 4: Yep. I don't know that there's a single answer, but I think it's interesting to think about those relationships between inside and outside the box and how that influences who decides.

Speaker 2: Yes. And again, that that, you know, that in itself is is is is, you know, a good a good example of the the fractal nature of this problem. In in any good fractal problem, the best you can say is that it's turtles all the way down.

Speaker 3: Yes.

Speaker 2: And and I I tend to try to focus on this context, the one above it, just in terms of scaling, and the one below it, and assume that the other parts I can ignore unless they force themselves into my model.

Speaker 4: Thank you.

Speaker 1: Thanks. Rick, I also see you put in a link, but it was also a question. So I'm wondering, are you also asking that question? And do you wanna unmute and ask it?

Speaker 5: Yeah. I actually, it's a compound philosophical question, and you can add to it and whatever. It's a concept that I've been working with for some time because whenever you frame a question, you're excluding something. The nice thing about a compound question, particularly when you use AI, you can throw this chicken sink into it and see what comes out of it. That's where, you know, fast AI can make connections or generate new connections that you can't make, that quickly. The flip side of that is, you know, something that I think you're alluding to, which is the the slow thinking human intelligence of, well, how can we how can we create co intelligence to actually elevate our collective intelligence and the wisdom of the crowd to break through the ceiling of sacred phronesis or practical wisdom. And I think it's that sort of creating these communities or networks or ecosystems that are aligned to be able to humans to remain in control of AI to do good rather than bad and the ugly. So, I mean, this is a question we could spend have a separate session on if you wanted to. But I'm just raising the question, not that you're gonna have time to answer it, but that's the whole point of a compound philosophical question, is to open generative dialogue about how to manage your wicked problems. So reaction to the question without answering it.

Speaker 2: The the the my my my request would be, can the the schematic that that was posed here for for sort of this three part type decision support system. I mean Can that be applied in the situation that you that you're that you're mentioning? As accepting sort of the the fractal nature of it without relying on that fractal nature as a crutch. And I think the the the answer is, you know, it seems to be yes in that I think the the the article that I pointed out with with the the the Ellie Rennie at all, They're moving in that it said they used it for that that kind of purpose. But the the proof is gonna be in the pudding. We're we're in the middle of an experiment right now whether we like it or not.

Speaker 5: I mean, my reaction to what you were you're describing was it was almost like a a a a horizontal super level where humans are constantly involved at at guiding it and, affecting the, the the what I would call it, equity meta governance of the system. And if people don't have a set of principles that they're agreeing to adhere to at a metal level, then it's difficult to actually, create alliance alignment and galvanization across the system, if that makes sense.

Speaker 2: Yeah. I think I think that's that's where the the middle layer is is quite quite important, the knowledge processing. The the the the the first layer, you know, sort of information processing, we can look look at the at the results of it and check off whether we believe it or not. It it it's we we do it all the time. It's it's just a thing.

Speaker 5: But my my my my my quick response to that, is there a way of systematically rather than waiting, is there a way of correcting the misinformation that any system will have?

Speaker 2: So that I think yeah. That that comes into what what does it mean to correct the system? All I can do is I can I can correct my decisions? I can correct the signal that I'm saying. This this is my control signal. You know, my room is too cold. I want it to be hotter. My control system turns on the the the the heater. If if that heater has a response time of twenty minutes, then every observation that I make before that twenty minutes is up is superfluous. Nothing has happened. I can't it doesn't whether the temperature has gone up or not has nothing to do with the decision I made. I had to I have to wait until this refractory period is over, and then maybe it it succeeds back. So there isn't there there isn't a full you know, there isn't a separation there. I can't always make that, but I should you know, given the given the system that I'm describing, I should be able to tune the parts so that it it behaves well enough for me to to use it. And it becomes tuning process rather than than a development process.

Speaker 5: I could go on, but I'll let other people ask questions. I could spend the whole next twenty minutes talking about this, but that's fine. Let somebody else chime in.

Speaker 1: No. I understand. I think it kind and then, like, semi tapping in more specifically to what you're talking about, I think from the context that I'm coming from in or like one of the contexts that I'm coming from in research, there's been this big kind of move that is coming around, oh, like a lot of specifically in The US, a lot of government funding has retracted, a lot of philanthropy has gotten scared as well, and there's also this gap of universities now also not being able to take as large cuts of grant funding and things. And so as a result there's been this massive retraction in kind of funding for a lot of research. And one thing that you mentioned in the presentation was this opportunity to be able to allow smaller groups working at smaller levels in knowledge processing to actually compete with some of these larger institutions. So I'm wondering if you're able to maybe build that out. I don't know specifically in research or if you have a specific use case or, like, example to make it really tangible. Like, how does this like, what would it look like to co scale, yeah, the human and the machine in a way that these smaller groups are actually able to compete with or have sufficient infrastructure to do the work that they need to do when a lot of the larger groups are actually not functioning that well right now?

Speaker 2: Well, right right now, I think the smaller groups will have to take up the slack because they have to. Unfortunately, you know, one of the points I tried to make was that the the fact that there are these massive organizations, may be a a function of the way decision support systems work. That in order to make predictions, you need a lot of latent space. And if that latent space is staffed by people, it's really expensive because it's staffed by expensive people. You can't afford it if if if you're if you're small. So good luck. But, you know, fortunately, may maybe the maybe the we've cut off the air supply to to big things. This will be like the meteor event that allowed mammals to take over after the dinosaurs. We'll we'll see how it goes.

Speaker 1: Great. Any other questions?

Speaker 5: I can certainly invite you to extend on the notion of cointelligence, because to me, that is where the secret sauce is gonna come from. Maybe you disagree with that. Fine. If you do, I'd love to hear your disagreement. Yeah. But how can how can we I mean, I'm finding that I I I actually splurged and got my perplexity AI $200 a month thing just to see, is it really worth the the extended deep research? And I can't say at the moment. It certainly generates three times as many references for the inquiries I'm making. And then I I'll just give you some things that I do is that I'll take the same question, generate from diff you know, you know, different AI engines and see how they compare and contrast in terms of their outputs. And then I've been using NotebookLM to create additional learning assets that are for generating dialogue, not as a definitive, you know, end or it sort of sets the stage for people to become activated to an inquiring mindset. So it's not gonna provide you the answers. It's just gonna make it it'll always the flow of ignorance. Let's put it that way. Because if people do look at the learning assets, they're able to sort of get a little bit more informed. And when they come together in micro communities, they can have a more informed discussion. But I think that's what we're you know, I academic institutions are struggling with this. You know, how how do they use AI more effectively? And I belong to an AI group where this is a subject of conversation is, well, how can we do that much more effectively so that it's a true partnership? And people have to demonstrate their partnership. But it's not just a question of going in and, you know, asking AI. It's how you do put the context of the question, how do you ask AI to ask questions, and show it as an iterative process. And then you demonstrate the competency of your AI skills to generate ideas, output, or whatever that's a part that demonstrates that you're actually creating synergies. So, I mean, I I I'm I just have a friend who's working in university at the moment, and he says they're in a quandary, you know, about how to enable students to do it, faculty struggling on how to optimize use of it. So this seems like a a fact of development issue that that people are are working on. I'm happy to connect with anyone who wants to who wants to address the issue of how we can really get the best out of AI. Well,

Speaker 2: since since I've I I used to work in ad tech, I'm gonna I'm gonna give a plug to the company I currently work for, BlockScience. Actively researching that, and have some some some good stuff to show for it. There's a block.science block.block.science website. It's got a blog. Lots of good lots of good stuff there. But, personally, you one one has to I I I think we're beyond the point where we can consider ChatGPT an application program. And I say that because we we require some stability from the platform that we're asking questions of and getting answers to in this kind of natural language mediated way. For people at work, you talk to a person. People have personalities. They have stable sort of properties, And you can get used to talking to them. You can you can develop a sort of a way of communicating between. That same thing is going to have to happen in some way, shape, or form with an artificial intelligence. And it won't happen if the underlying architecture on which these chats or this generated content is being generated keeps changing every day. There needs to be some stability. We we've we've we're we're you know, I I I I would look strongly that your concept of self infrastructure in order to sort of ensure that level of stability. It's it's it's it's, you know, it's probably not going to happen because it's much easier to get something as a service. But then you're talking about service level agreements, and there needs to be something in those service level agreements that specifies stability. I think that that's that that's a that's a key for for moving forward with it.

Speaker 5: I'd like to maybe explain the meaning of stability because, say, I the flip side of that is variability. That there you can have healthy variability, not necessarily yeah. So, you know, when I do those multiple search of different engines, they give me different perspectives and different ways of looking at things. Mhmm. So, you know, what what is a terrible level of variance provided? Well, I mean, as long as there's not misinformation there, then having multiple perspectives in dealing with wicked problems is incredibly helpful.

Speaker 2: I think I think we can look at psychology. We we we we we psychology has sort of looked at the continuum of of people who we have we have judged to be in contact with reality and not in contact with reality. And they've produced all kinds of scales and interesting ways of judging which which side you're on, and why you may not be on one side versus the other. Similar kinds of things need to be need to be developed for for these models. And I think it it it it's much more working with a with a with a generative model is much more like hiring an employee rather than buying an application.

Speaker 5: I agree.

Speaker 2: And you you can't assume that a that a high a new hire is gonna show up and be able to do

Speaker 5: Exactly.

Speaker 2: High competent level everything you need them to do. You have to train them.

Speaker 5: Exactly. Absolutely.

Speaker 2: And then once you've trained them, you hope that they don't, like, run off to another company because they get paid more, or they get run over by a bus. All of the things that we, you know, people who have had staffing concerns have worried about. Mhmm.

Speaker 1: A 100%. Well, as we get into the end, I wanna give, like, one last call for any for one final question. And if not, I wanna do a huge thank you to David for coming today and for doing this conversation. I think this was really it's always really lovely to get into the depths and the details of, like, alright. What is it actually when you're dealing with information or data or all of these things, which we are all dealing with increasingly? And then with that, I'll do a short closeout. We have, not next week, so if anybody wants to present or propose a seminar, feel free to do that. But the week after, we have another seminar here with MediGov. And then in a month from now, we'll have a seminar with Elif Ekmektsky, who is a Turkish researcher. She is focused specifically on the ethics of AI. So we'll get to explore a little bit more about how we're gonna use some of these systems, maybe see if we build out these implementations, and how do we define it based off of, yeah, planetary and human boundaries that need to be respected. So, yeah, huge thank you to David and everyone here for all the questions and engagement. And, yeah, everyone can we can do the short thank you clapping moment. Oh, it was really beautiful to hear this.

Speaker 2: Thank you.

Speaker 5: And, Devin, I sent you a private chat. I I sent you a private chat if you're curious. Okay?

Speaker 2: Thank you.

Speaker 1: Thank you all. Bye.

Speaker 3: Bye bye. Keely.