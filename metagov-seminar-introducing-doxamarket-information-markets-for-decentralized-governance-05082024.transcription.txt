Speaker 1: And

Speaker 2: alright. Wonderful. Hi, folks. Welcome to oh, gosh. I stopped counting a long time ago. One hundredth plus, episode of the Meta Governance seminar. Today, it's my pleasure to have David Zhou. I feel that's the Chinese pronunciation. Zhou?

Speaker 1: Yeah. Yeah. Very correct.

Speaker 2: Busting out my way, my ancient Chinese skills. Welcome, David, to the call to talk about his new project, Doak SubMarket. No. I really don't know whether I'm pronouncing that correctly. But, actually, I I don't really know that much about David. We just hung out at ETHDenver Yeah. At Funding the Commons, and I really liked what he was doing. And I said, you should totally come by and talk about what you're doing with these prediction markets. And I think that's exactly what he's gonna be telling us about today. So welcome, David. And if you wanna give you a self a little introduction, that'd be awesome too.

Speaker 1: But Yeah. For sure. For sure. Hey, everyone. I'm David. I'm a master's student at USC. I'm graduating in twelve days, rip, but we don't have commencement because of the Palestinian protest, but it is what it is. I'm from block I'm part of Blockchain at USC. I lead the governance team there. So we've been a delegate at Optimism for almost two years. Thanks to a 16 z's generous tokens. Before that, I was at UC Berkeley. I was studying econ and political economy, did some VC experience, startup experience, and DeFi, and various sorts of stuff like that. Later on, USC master was mainly focused on data science. So and graduating in twelve days, I'll be full time full throttle this prediction marketing. I'll work. So you'll hear me rambling about that a lot on the Internet. Alright. Let's get started. I'm a oh, could you allow screen share, Josh? Right now. Alright. Cool. K. Yeah. Awesome. My screen. Yeah. Okay. Awesome. So so, yeah, welcome, everyone. This is not Doksa. It's Doksa. It's fine. Doksa in Greek means, like, public popular opinion. And, thank you really much thank you so much, Josh, for inviting me to share this opinion, idea with the MediGov seminar, folks. And so let's talk about some prediction market use cases in real life. So most of you all probably know, like, a poly market where you can use prediction market to see the US elections. It has been proven, like, somewhat more accurate than, you know, surveys and stuff like that. So we won't be talking about that. That's obvious stuff. What's some of the more important stuff is one thing is called the forecasting. So Jamie Dimon have you know, has previously said, like, I always told the fact to stop forecasting because cause it's never accurate, and they just make up their own numbers for any kind of indexes. So they've they found, like, you know, there's 8.3% differences between, you know, the yeah. The actual index prices to the price targets that analysts set. And there's a shit ton of conflict of interest inside the Fed, inside the analysts themselves, and then maybe prediction market can be helpful with, like, crowdsourcing these, you know, information for economics. So recently, there's this company called Calshi. They're a regulated entity. They acquire some CFTC and some kind of licenses from from The US. They also onboarded the biggest market maker inside TradFi. It's Sus Kohana or something like that. But what they did is they provide the liquidity, and they provide traders or forecasters to predict on the results. And, you know, it it shows some progress, actually. So, basically, what they did is some comparison between the actual forecast and I mean, the actual earnings report versus the financial analyst forecast versus the. So you can see is all metrics, whether it be Tesla, net Netflix, Spotify, Meta. They've been closer to the actual truth. So we'll continue to monitor how how they operate over the time, but at least for now, it's been shown that it has significant use in at least economic forecasting and, stuff like that. So another use cases for prediction market is used in scientific paper replicability. So, basically, researchers have submitted a lot of, you know, research to publication in the past, but some of them can be replicated, others are not. And there's no good ways of doing it effective and scalably. So one way that people come up with is maybe let's just use a prediction market, and let people bet on whether or not this, this this this research can be replicated. And the results is, you know, quite stunning. Like, it's success successfully predicted 71%. Of all researchers out there, the accuracy is quite quite high. And if you compare that accuracy with, like, traditional survey method, and it it it still has higher accuracy than that. So market based approach in whether or not it's in economic forecasting, whether or not it's in science science field, and prediction market itself has shown some progress and use cases in it. So jumping off to today's topic, I wanted to introduce prediction market and decentralized governance, the mesh between the two and how it might be able to help with, some of the problems inside decentralized governance. So just a primer on optimism in case y'all are not familiar. So optimism has two house. One is the token house. The other is the citizen house. They all have, like, different functionalities. Token house is basically, are one the more tokens you have, the more voting power you have, and they vote on various proposals like protocol upgrade, project, incentives, treasury funds, how to allocate them. And there are various councils inside, like, and decapture commission or, grants council, all all sorts of stuff like that. And for for the citizen house, they are mainly focused on retro public goods funding. Whether or not it's public goods, this upcoming round, well, we'll see how that goes. But at least in the past, it's retro public goods. Retroactive means, like, you look past on their past contribution or, like, what whatever they have contributed during the during that period and give them a some kind of a reward based on their contribution. It's not a forward looking grant. Forward looking will be more similar to, like, a VC type of investment. But, anyways, I would categorize, grants council and citizen house both in, under the resource allocation, bucket because they are mostly focused on how to allocate the OP tokens to projects efficiently. Okay. And this is one of the problems that Docsomarket are solving, and we will be focusing on that today. So more background on how grants council deliberate. So the recent grants from optimism has three phases. So start its application intake, then preliminary scoring, final review, and the announcement. So you'll see on the graph, it's like a step function where, initially, you have a lot of projects. Over time, it shrinks down to a few projects that that get selected. And for the prediction market, these different, these different time period will matter a lot, in the prediction market because from application from the end of application intake to the end of preliminary scoring, this period is, basically, what what's under the hood is all the grants council people independently assess, different projects and give them a score. Now at the end of the preliminary scoring, they will release a a final score for all the projects that applied. So if that score is public, then the prediction market should not should not operate because then people will bet on the last minute, yada yada. It it's gonna it has data leakage inside. So, basically, what I'm trying to say is this prediction market should only start from application intake to the end of preliminary scoring. We don't allow people to see the final score released by the the grants council people. And then during the final review, amongst these scores, they will pick a cutoff, and then they choose whatever projects meets the criteria. So yada yada. Alright. So this is the first market oh, Josh, you have questions?

Speaker 2: Okay. I'm just gonna interrupt briefly because this is just a clarifying question. Could if you go back to the slide before, is this effectively the the process that's, like, implemented on CharmVerse? Yeah.

Speaker 1: Pretty much.

Speaker 2: So it's exact it's basically just the CharmVerse process. Right?

Speaker 1: Yeah. It's a CharmVerse process. Yeah. Three phases and then stuff like that. Yeah. Yeah. Thanks. Okay. So, mechanism design for the first market, we call it Didomi. It's a very simple market. We have static pricing. Sorry about that. We have static pricing, meaning, like, there's no dynamic, price of a position. You would just deposit money inside, and then the minimum deposit is, like, a 100 e gen, token or something. So how it works is, as I said earlier, for the insiders, you can imagine them being, like, the grants council people. TBS is t betting star, t betting ad, and t final. So grants council people start reviewing these projects independently. Alright. TBS from t betting ends is reflecting back to application, start of application intake to end of preliminary scoring. So during the decision period is betting ends and before the t final is basically this period from end of preliminary scoring to the final announcement, basically. Why I mass why why I don't allow people to participate in that because I explained earlier, during this period, there will be public scores revealed, and based on these scores, there might be information leakage. Alright. So after the, you know, final announcement is, given out by the OPE grants council, we will resolve the market. So how does the market work? Right? So during the review period, we allow anyone, okay, anyone right now, because we have no tractions whatever, so I don't really care about insiders, to be to be honest. We allow anyone to bet on which project or projects will get resource allocation. Very simple. And they'll have an aggregate pool of all the bets inside. And then during the decision period, which is the, which is the period from final review to end, there's no bets. And then when announced, there are losers and winners, and then losers pull or distribute to winners. Simple as that. And then there's no dynamic pricing. As I said, the minimum deposit is, like, a $100. Basic basically, you can think of it as, like, a deposit contract and then redistribute it. So some results. We're still waiting for the optimism grants council's final announcement, but based on the current result. Right? So for the preliminary so they would announce this scoring. Right? And then six out of 10 projects that are listed that that are vetted on DocsToMarket are the project listed on the the preliminary scorings. So let me just show you real quick. These projects on on the left here. These projects are selected. At least they are listed on their their official, what is it called, Google Sheet, and they're considering which of these project will get final allocation. But, basically, the the gist is there are 30 projects in total, and they eliminate down to 13 during after the preliminary scoring is announced. Now they're in a deliberation process, which we will know in either twenty four hours or or less amongst the 13. How how many will they choose? Okay. Going back. So at least for now, there are some promising signals inside the market where we have just under 10 people betting inside, and then the stake is really, really low. But if people put out their effort into betting stuff like this, the result is, at least for now, it's decent. Okay. Going forward, any okay. Let me check if you enter any questions. Okay. No questions. Perfect. Okay. Going forward, I'm gonna talk about public goods. So what does how does public good deliberation process look like inside optimism? So badge holders, which is the smiley face, each badge holder will choose amongst a set of an impact metrics. These impact metrics are definite publicly defined quantitative metrics. For example, it could be on chain metrics where, like, which contract is generating the most revenue. It could be off chain metrics such as GitHub repo, commits, or, like, you know, whether you you are a full time dev or not, stuff like that. So the first step for badge holders, which is the part of the citizen house to determine, what kind of, public goods that he or she or they should fund is selecting, some of the impact metrics. Second step is assigning weights to these different impact metrics, and the weights add up to one. That's the constraint. And then, lastly, all the batch holders come back together. We aggregate all their results together, and you'll see a curve or something that looks like this. And then they'll assign a cutoff based on the resource allocation constraint. They only have 10,000,000 OP or so to allocate. And then based on the scores and stuff, they'll set a horizontal threshold saying, like, these are the projects that that made it. Anyways, so during these three processes, there are biases, human bias in in this process. How do you choose the right metrics? How do you choose the optimal metrics? How do you assign the optimal weights? These are biases that will get introduced by each different batch holders. And I would my hypothesis right now, we haven't built this out yet, but maybe we can use the similar architecture for Didomi to use it for, retro funding round that is coming up. Why? Because the optimal weight selection and the metric selection, there's no game theory optimum right now for any kind of batch holders. However, the prediction market results will somehow let let the bad holders know somewhat in the range what is the weights well or, like, at least, what's the range of the optimal weights that they should choose? But this is this is, like, totally up for discussion and research. We we don't have a final answer to to this yet. But if we set this market up, it will be very something very simple, like, which project will get the highest funding. In the past, since in the past of the optimism retro funding round, it has been, like, solidity or some very open source project being, like, the top one. However, this round, they are focusing on different buckets. For the very first bucket of project, it's focused on, like, which on chain builder is the most active, like, for Superchain. These stuff are vague, unless, like, Jesse Pollock just somehow pulls up, like, the founder of Base, then there's probably a no need to set up who is gonna win, like, the this award. Right? So if it let's just say if this, retro funding for the first round, like, the application wise is is vague and then you don't know, then we'll just ask the very simple question, who's gonna win? Who's gonna be the the top, builder in the space? And and yes. So, obviously, this current approach has a lot of problems. The very common problem for any prediction market in general, whether it be Polymarket, Cauchy, and a lot of other markets out there is liquidity. Like, how do you make sure there are liquidity? There there are, your your counterparty insight. Right? Like, that that's a huge issue. That's why Kauschiy made a deal with Susquehanna to let the market make insight. Polymarket is already big enough. They have their depth of liquidity. Now that's solved because of their brand, their mold. But for us, we have to think about ways that we can get the liquidity inside. And then another is just how do you make sure the participant can exert effort into these forecasting competitions or, like, markets like this. Like because to be honest, like, the first market that we implemented, I feel like it's not easy to play for participants. You have to look over at least half of the projects to know what they're doing, and then you can start betting on which project will get resource allocation. Now, like, that's only for 30 projects. Retro PGF is, like, the past run has, like, 800 projects. So the scale of the difference between the two is, like, vast. And we think either we provide people with a a a more money monetary incentive to participate in this might work. And then second is how can we maybe we if if we are able to demonstrate that there are information value inside this prediction market, then that's our mode as well. But for now, we're I'm just rambling, and then we'll we'll let the market validate whether or not these hypotheses are true or not. But, so I wanted to just briefly touch on AI agents, because the the relationship between agents and markets has been very prominent in stocks market, in crypto marketing, any kind of equity market. Like, 90% is AI algorithm, bots trading, 10, 10% is retails. So that's why I'm thinking maybe the majority participants in the future for prediction market should be the AI agents themselves. And why is that? Because agents have all the resources and effort and bandwidth in the world to analyze, let's say, a thousand projects in one go, and they can analyze it over and over and over again. They can update their results based on Bayesian thinking, update their probability. And then over time, they're able to come to a conclusion that, hey. These are the projects that might get resolved might might get selected by by by the market, and then they'll choose choose these. So if anyone's building any, like, AI agents or agents in general, please ping me in the Slack or contact me later on after this seminar. Very interest very, very interested to get you on board and see what you need for for you to plug your agents into prediction market. Okay. And oh, question, Steve. Go ahead.

Speaker 3: It's just that I am working on AI agents, and I don't think that you need to have that type of solution. In fact, I have a retro goods funding solution myself, which Eugene is also familiar with. And actually, I think this would be great to pair with that, but I would use the predictive markets very, very early to get eyeballs on small projects. I do it use it in kind of a different way. So I'd love to have a conversation about it. But, yeah, I don't think yeah. I think and and my system, everyone doesn't have to know about every project that's an advantage of it. So

Speaker 1: Yeah. I totally agree. I feel like yeah. Just to, answer the question of how how we can maybe different, you know, design it differently for public goods. One way that I'm thinking about it is maybe we should use prediction market to let people bet on which public good should people even pursue. Like, that's valuable instead of, like, you know, coming, laying around and then see which one will will will get funny. But what I'm

Speaker 3: like Exactly. But I I wanna see, like, a place for, like, evangelicals within the community. In other words, someone who can say some this is this is gonna get funding and then go out to the community and promote it to get at that funding. You know? I want it to be a function rather than a bug.

Speaker 1: Mhmm. Mhmm.

Speaker 3: Got it. Yeah. I'll talk to

Speaker 1: you after the after the seminar. And, yeah, we'll we'll shoot you a message for sure. Thank you so much for the questions, actually. Alright. Going forward, this is all the presentations that I have. For now, I'll open up to any questions for now if yeah. For now, I'll open up to questions, and I have some group activities, actually.

Speaker 2: Awesome. Thanks, David. One additional quick question for me is just so maybe I just didn't catch it. So in the example you have for, like, the the the advanced council, who are the people participating in your experiment right now in the production market?

Speaker 1: Yeah. It's mostly from our team and then some other folks that are interested in this in the governance forum. I don't think it's the insiders, to be honest.

Speaker 2: So Okay.

Speaker 1: Just under 10 people. Yeah. Okay. Okay. And it is carried so probably you all don't know Forecaster. It's like a decentralized Twitter. But, yeah, it's carried out inside Forecaster fee. Imagine, like, you're scrolling through your Twitter fee, but inside, it's all crypto people. So you open it up every day, and then you saw something interesting, and then you just bet on the projects. That's the whole process. Yeah.

Speaker 2: Interesting. Yeah. So just the okay. So the the actual votes are completely public. Right? You see everybody else It it like, you're voting through Farcaster or you're voting Yeah.

Speaker 1: You're betting through Farcaster, and you can decode the you can definitely decode the on chain data. Yeah. It's it's public if you have the ability to decode, but it's public. Okay. Interesting. Yeah. That's fine. And the way we're doing it right now is you you technically can dox the person who vetted it because we have their forecaster ID attached to it. So, you know I mean I mean, unless this person created a bot a forecaster account. Right? Like, they still have to $5 sign up for all that shit. But okay, let's just say this is a real person and you know their name. It it is possible right now. So Interesting. Yeah. Very interesting.

Speaker 2: I wish Eugene was here because this is something very, very much up his lane as we're talking about Grant's prediction. But I see a couple comments. Ofer, I don't know if that's a question or a comment or if you

Speaker 4: Yeah. It's kind of like something in between, but I was wondering about because, no, if you try to predict I know that when I was reviewing grants, it's quite easy sometime in the committee to predict which grants will get good reviews. But it's much harder to predict which grants are really good in terms of outcomes. And so the wisdom of the crowd here, and here it's a very small crowd, it will be interesting to test it also based on outcomes. That that's just a thought.

Speaker 1: Mhmm. Can you talk more about the you said instead of outcome, you said the feedback of grant.

Speaker 3: What do you mean by that?

Speaker 4: No. I I talked about funding. Like, what is the chances that people will give the grant funds that Mhmm. So it's easy to predict because, you you know, many times you look at it and see people are very good in writing and presenting their ideas in a way that are more appealing, but that's not a very good indicator for the how how good the science is, unfortunately.

Speaker 3: Mhmm.

Speaker 4: So many times when you both get funded by the NIH and NSF, and I did a lot of those. It's not really what I think. And, also, when I look retroactively, I I know I'm a bit older, so I'm looking, you know, what happened ten years later. And and you can see, you know, that many times you know? So so so the kind of thinking is that it will be good to to do something that is a bit harder, but to wait a little bit and say, okay. A year and two passed, but you'll make a prediction, you know, not if it did get funded or not, but if it gets funded, if the outcome will be satisfactory or not. And and that's harder or important, I think.

Speaker 1: Yeah. Totally agree. Totally agree. That's I feel like, right now, optimism is, milestone based. And, okay, let's just do one thought experiment. Like, say, I give you all the builders' past history inside OP, whether or not they completed milestones. Are they are they on time to deliver their their product? And say you have a database of different builders inside. Now you take that into any AI agents or just human judges, for this prediction. Would that be helpful?

Speaker 4: Yeah. And, you know, another thing that is can be done without looking into the future is actually looking into the past. For example, look at the New York Times articles. They get a lot of scores of the how how many times that they were mentioned in Twitter and all of that. But what is actually more interesting is if you go back and try to see, okay. If you look at this paper from five years ago that that said, okay. That's going to happen or this is going to happen, then you can actually judge if the article was a good predictor, and you can compare that to what people thought at the time because everything is not now based on the present and not enough on what will actually happen. And and and that's a a limitation of those tools. And and it can be addressed. There are many ways of thinking about making something that is a little bit more long term.

Speaker 1: Mhmm. Yeah. That's really interesting. Yeah. I'll definitely talk to you more about this afterwards. Thank you for the input. I saw Steve have some other questions. How are predictors awarded for making correct? But for the v one version, it's just it's very, actually, very simple. It's just whoever loses, they'll they'll they'll compensate the winners. Is that what you're trying to ask or something else? Steve, you there?

Speaker 3: Finding my unmute. Finding my unmute. So yeah. I mean oh, yeah. Right. I for some reason, I got the impression that it wasn't, you know, that type of market where there you did say it at one point that where there had to be a a counterparty for every bet, but then there was something about 10,000,000 things being distributed. So I thought you were sort of just distributing those in such a way as to predict what the amount allocated would be to each project based upon that somehow. I I I I missed how Yeah.

Speaker 4: Oh, you're totally

Speaker 3: that even happened.

Speaker 1: You're you're you're good. So just to clarify, any at least the market architecture that I am brainstorming right now is based on this graph that Didomi is set up. Basically, people bet on whatever questions that are interested, and there are options inside. And at the end, how market resolve is is there is a pool of losers and a pool of winners, and then losers distribute to winners. That's that's it. In regards to the market questions, it can be, like, who's the top or, like, what who is the top projects? Or it can be how much does the top project receive? Like, it it can be various sort of questions. But does that answer your question? Like, hopefully, that clarifies it.

Speaker 3: Yeah. Totally.

Speaker 1: Okay. Awesome. Rick, questions. So

Speaker 5: Yeah. I don't know if this question is relevant or not because I'm trying to get the gist of what you're saying. Earlier when you were talking about the issue of biases and predictions, And I'm wondering to what extent you've got built into it, you know, scenario planning where you're not actually predicting the future. You're putting in different variables to map out the range of possibilities. Is that does that resonate, or is it not part of your part of this model?

Speaker 1: Are you saying something like a Bayesian thinking processes, like, where you update your probability based on information that you collect at the moment?

Speaker 5: Partially, but it may be other other factors where there are confounding variables that you might introduce as something that might affect. So no. Just just in terms of creating a range of possibilities of what the outcomes might be based upon altering the factors that influence the

Speaker 2: the the

Speaker 5: the range of scenarios of whether it's sex you know, degrees of success, etcetera. Yeah. Does that make sense or not? Yeah.

Speaker 1: Yeah. Totally. Totally. I I actually like the idea. But, businessly, I don't know how scalable that is. I need to think more about it. Yeah. It's a it's interesting thought. I don't have a great answer for you right now. I I need to.

Speaker 5: Well, you know, one one way of doing that is actually is using the wisdom of the crowd. And I I I raised another issue about the voting, proportional voting, and where the different voting methods may affect some decision making processes. So, you know, somebody has, you know, 70% of the whatever, then they're gonna predetermine the outcome of decision making. So how do you I don't I I didn't hear enough about the voting aspect so that that Yeah. Protect against

Speaker 1: Yeah. Yeah.

Speaker 5: How do you have balance in this?

Speaker 1: Yeah. And to answer that, this this is actually the second market that we'll introduce once there are proposals inside optimism when they surface, but how we do that is very simple. If this delegate has a lot of votes, we basically run some kind of inverse bonding curve based on your the the more the votes that you have. So the more votes, the more the more expensive, basically, your position is and the harder for you to get in or something like that. So that's one way that we're thinking. And then the other is the time component if, you know, we don't want people to bet on the last minute, and we ideally want people to be aligned in their action. Like, if you bet on this, let's say, this outcome, you should ideally vote for this outcome. You don't I don't want you to hedge your outcome. That's not an opt optimal information elicitation equilibrium, but these are the stuff that we're thinking for the second market to come up. Alright. Josh, questions? Quick question.

Speaker 2: Do you have a sense of I was just thinking about, like, you know, there's different settings. I mean, you can establish prediction markets, right, and different kind of expectations about how efficient they're gonna be. Obviously, liquidity is a big thing. The but also, like, information availability. Right? Mhmm. Do you have a sense of whether, like, let's say, optimism in this case, right, or any perhaps any DAO or blockchain based network. Do you have a sense of, like, should it be a better setting for prediction markets? Would you expect prediction markets to function better there because of, I guess, presumably, maybe more widely available information? Or do you actually expect it to kind of function worse relative to other, you know, let's say, like, political kinds of prediction markets? Because, like, maybe it's just, like, too too much smaller.

Speaker 1: Like, how

Speaker 2: would these effects how would you weigh them in your head?

Speaker 1: So so the ideal prediction market for me to run it on decentralized community is I have all the people in the world to participate. The sample size should be equal to the population size. It should be big enough so that the wisdom of the crowd will work. And then that's one. And then second is how easy is it to to play this game? Is it do you need to exert a lot of effort into it? Is it easy to play? And then they're like, if the duration span of this market is too long, Would people be able to withstand that with people, you know, you know, turn from turn away from it? These are the three vectors that I'm thinking constantly. How can I find the optimal combination? But in terms of, like, which community might be best, I I think OP historically has been very open to governance experiments. So that's the first quality that I seek in communities that I wanna set up prediction markets on. And then based

Speaker 5: on

Speaker 1: the grants feedback, they're not against it, which is already surprising. And the grant is already ranked so high amongst all other projects at the time, so I definitely think they're interested in this. So, like, in terms of other communities, maybe they are, maybe they're totally against it. I can totally see that because it's very manipulative if if, you know, the incentives are not aligned perfectly. And if we bring it to real world, you know, Google, Microsoft, all the big companies, public companies out there, they all have their own internal prediction market set up in the past. And they they had various learnings. Some are too optimistic of results. Some, you know, encourages insider betting. But overall, like, it has improved, like, their corporate decision making over time. I don't know if this rambling helps with your question, but this is what I got. Yeah.

Speaker 2: I think Brett is next. What's the question?

Speaker 1: Oh, you're muted.

Speaker 6: Hey. Sorry. This is all outside my normal area areas that I'm in. So it was super interesting presentation, by the way. I think you did a great job, and I feel like I learned a ton just a lot. Like, listening, I feel like I had learned a lot more. So my question may make no sense or it may just have an obvious answer. I don't know. Listening, I kept thinking about what kind of generalizable knowledge is you can you can take from unsuccessful or successful predictions. Like, you know, we're at at the end of the day, does the market are we learning something about the decision making process? Are we learning something about the quality of the grants, for example, or are we learning something about the predictability of the likely outcomes as someone was saying earlier about

Speaker 1: Yeah.

Speaker 6: Success rates in doing the public good generation down the road?

Speaker 1: For sure.

Speaker 6: Because I can't I I can't help but be thinking about, like, how to leverage how to, like, leverage this kind of mechanism to solve all kinds of different public goods problems, whether it's content moderation online embedding about what's fake and what's not, or it's or whatever. Like, you can imagine where I might Mhmm. Be going. But Yeah. Yeah. I just need to know a little bit more about

Speaker 1: For sure. Yeah. For sure. So the current v one version that we set up is the value that we can extract from the market outcome is basically twofold. Onefold is if the market outcome is, like, totally against the grants committee's outcome, then the grant committee might be doing something weird. So you can treat the market outcome as some kind of a a almost like

Speaker 5: a Like

Speaker 1: a check. Grading system to to the performance of the people inside the grants council. Did they fat finger? Did they make misjudgments? It's not perfect, but at least, let's just say, like, there's one project that receives 1,000,000 bets. Okay? And somehow we are able to let's say we cap the amount of people and the amount that people can bet. It's obviously the pop populous choice. And why does grants council choose against that? They probably need to answer that if given the governance space out there. And if they might be convinced, then we can put up a vote inside the optimism community, say, we should cons reconsider this project because it's a popular vote or something like that. That's one fold. The other fold is if they are, like, perfectly aligned, like, a b c project versus a b c project, like, both sides, then the takeaway is, do we need grants counsel at all? Right? Like, maybe this can replace grants counsel in in some ways. You know? Maybe not totally. It can be a useful tool for them to make better decision making processes. So at least that's for this very first market that we are thinking. And then second, the second one that for the public goods one. This one is more nuanced because I wanted to use prediction market to solve the human biases inside batch holders judging processes. So as I said earlier, different batch holder choose different set of metrics and different set of weights for these metrics. How do you know you choose the most optimal and fair one based on your preference and based on the badge holder community preference? Right? Like, maybe maybe prediction markets, final results can influence back to your thinking process of assigning weights and selecting metrics. If if that answers your questions at all, like because because currently, like, there is no objective functions inside governance or public goods. I'm basically trying to give this unsupervised learning process if you're familiar with, like, data science.

Speaker 6: Yeah. What like, what's the relationship between predictions and preferences, for example? Or is there one? Like, it seem it right? So, like, beliefs upon which one makes predictions about outcomes is slightly different from, you know, different weights I might assign to the value of different outcomes. Or maybe that's not exact maybe I'm totally misunderstanding what the No.

Speaker 1: No. No. If if you're saying I I think you're saying it from two different perspective. Preference are badge holders' preference. They ideally don't bet because they're the insiders, but if they do, like, they probably gonna reflect their own preference by the weights. Right? But prediction wise, it's the public. Obviously, we want to use the bets to show that it's their skin skin a game. Like, basically, they're betting on their own preference. We don't want them to hedge any any stuff. Like I said earlier, it doesn't, you know, do well to their prediction market itself. But does that answer your question? Like, I hope Yeah.

Speaker 6: Yeah. I I was mixing up the

Speaker 2: Okay.

Speaker 6: The bettors and the badge holders. Sorry.

Speaker 1: No. You're good. Alright. Any alright. I see one. I think individual optimism will skew any but without it, it cannot participate in the yeah. Yeah. When knowledge is generated by n six okay. Okay. I think we answered all of the

Speaker 2: questions. Just maybe, like, to tag something that and that's a little bit looking more forward. So you're interested in, you know, testing prediction markets on retro PGF. So it's like a big funding cycle. Right? Like a big funding cycle also in Optimism.

Speaker 3: Yeah.

Speaker 2: Have you thought about other places where you want to it is it sounds like what you're trying to do is, like, take the the general tool, the toolbox of prediction markets, and apply them to different use cases, generally speaking, within Web three, perhaps attached to kind of governance decision making, in this case, making decisions of grants.

Speaker 1: Yeah. Ideally, I want to introduce that to any community that's willing and have an open mind to test it out.

Speaker 2: So my challenge for you is, like, we run lots of experiments a minute ago. Right? We have an AI experiment that Okay. I actually, I don't know if everybody has heard of the koi pond. It will be launching relatively soon, actually. But what would be an experiment that might be relevant to this community? I'm kinda curious. Just a little bit of, like, a challenge for a brainstorm. What would be Are there

Speaker 1: any, like, grants, say, resource allocation problem in simedagov that you found, like, just painful?

Speaker 2: Well, we do have, like, the the interrupt grant that just got released. Right? Okay. Interrupt. Right? Okay. Otherwise, we have, like, things like what, literally, the the seminar selection. Right? But that's usually a nomination, and that just kinda tends to go through because the people who submit, you know, generally know what is interesting to this group. But it might be interesting to ask, like, you know, what are kinds of conversations, what are kinds of seminars for things will get the most views. And there's a lot of different things. The point is, like Yeah. I I think you wanna experiment with. I wanna maybe just put that opportunity out there.

Speaker 1: Yeah. Like, for seminar nominations, how do we how like, tell me if this is a problem. Like, how do the community members know what kind of, you know, seminar they they want to know in the process? Maybe they can. Instead of, like, signaling, they can stake something. Use stake as some kind of, like, betting similar incentives to to show their, like, true intention or preference in the process. And if they stake it, they kinda have to come. If you don't come, then we can slash away from it. It's it's, like, negative incentive, but, like, it's it's been, like, pretty pretty effective in, like, what is it? Mochi or something something like that, like like, where they allow, like, co working, like, community monitoring each other. And then if you don't do something, then we slash it. So maybe that's something we can do. But for interop grants, like, what's the deliberation process? Like, like, I need to know more.

Speaker 2: Maybe I mean, the interop is gonna be, like, a, like, a relatively closed system Okay. Or closed decision between, like, probably five to 10, you know, re grant reviewers.

Speaker 1: K. And then how many projects will, like, historically, which is say,

Speaker 2: like, give me We don't know.

Speaker 3: Give me supposed

Speaker 2: to be running. You don't know. Okay. Yeah. Mhmm.

Speaker 1: Then okay. I was I was thinking, like, maybe this this this tool might be able to help with MetaGo's, like, future higher stake decisions in the process. Like, say let's just imagine okay. For Tesla okay. Let's just say Tesla has a community of shareholders. Okay? And they decide on which which which country that they should open their gigafactory at. Whether it be India, like, Iceland, somewhere in Europe. I don't care. Like, four, four, four countries in the world. Yep. How do they decide that they can basically, like, stake their belief? Like, just, I think, you you know, it's gonna be in in in in Finland. Okay. Some somewhere like that. 1,000,000 people stake their Tesla shares in in there. And then the rest, like, 9,000,000 stake elsewhere. And then when the market ends, like, let's just say the Finland one is is the winner out there. And then you don't really have to have monetary incentive. Like, it it doesn't actually have to be the tokens itself. You can say, like, oh, thank you guys for doing this. Here's a small incentive for you, like a small bonus. Thank you for the information value that you contributed to Medigov's community or Tesla's community. So, like, there's a lot of ways that we can play this. Like, it it can be play money. It can be actual money. It can be, like, money come afterwards if you see the information value. Like, let's definitely talk more about this if you're like, if if something comes up to your mind about specific processes inside MediGov that can be improved. Yeah.

Speaker 3: I just I like the idea of it being play money, and then, like, maybe accumulating month to month. You just get more play money, and you have to bet it or you lose it, essentially. Yeah. Exactly. And then you, like, have everybody's global reputation decay, like, 10% a month, but then whenever you win gets added to that. So you sort of have a sort of a prediction reputation score that, you know, you can sort of

Speaker 1: Yeah. Exactly. But, you know, is doing

Speaker 3: that. Yeah. Yeah. Exactly. Exactly. You you

Speaker 1: have, like, a forecaster professional forecaster status inside that account. I saw a question, Rick. Yeah. David, I

Speaker 2: think Before we get to the question, we are at time. So I wanna actually just please ask and, you know, if David can say it a little bit longer, we can we can Yeah.

Speaker 1: Yeah. For sure. Yeah. I'm free.

Speaker 2: Please, everybody, as is tradition, unmute yourselves and join me in a raucous round of applause for David for a wonderful presentation and conversation. Thank you so much.

Speaker 3: Another, it's his audio just cuts out. Yeah. Cut.

Speaker 1: Thank you, guys. I

Speaker 2: have to jump, but, David, if you could take a round for a little bit. I'll turn off recording.

Speaker 1: Sounds good.