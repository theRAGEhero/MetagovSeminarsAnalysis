Speaker 1: Awesome. So hi, everyone. Welcome to Medigov seminar. Today is Wednesday. What is the date? September 4. It's noon eastern time. We do these seminars weekly. This is an opportunity to have a researcher in the governance space generally present us. We invite folks to propose others to come and share their research. So today, we have Denisa Cara, who was recommended by Coraline Ada MK, who saw Denisa's work at a conference and was like, oh my gosh. You need to have Danisa come to MediGov seminar to speak all about agent based simulations, experimental governance sandboxes, and it was a no brainer. So thank you all for coming. The process for this session will be about a twenty ish minute or so presentation, and we have a little bit of a demo. So we've made it given Denisa a full twenty minutes to do presentation and demo for us, and we'll have time for a q and a at the end. Throughout the presentation, please feel free to type your questions in the chat. And so you can remember them by the time we get to the end of the presentation, and we can ask Denisa our questions. And please wait your turn. So I will curate the questions and the order list that they come in so that we'll give folks, you know, an opportunity to ask and get their questions answered, and then we'll move on to the next question. So I'll be in charge facilitating that, about twenty minutes from now. So, yeah, with that, I introduce Denise Acara, who's going to be, giving us a talk about their research and a demo as well. So this is experimental governance sandboxes and LLM based agent based simulations. Welcome, Denisa.

Speaker 2: Thank you, Val. It's a real pleasure, and I'm looking forward to seeing some of the previous presentations, which I didn't manage to see in real time. So So I'm really excited. I love the term meta governance because that I feel it fits well with some of the agenda I have. So I'll start sharing my screen, and maybe I will actually start with the with the small demo where why did I start doing this. Maybe it will explain why did I start doing this agent based simulations. I hope you see it. You see my Visual Studio? And Yeah. Yeah. It's a very simple let me put this some way. So it's a very simple for me, it started with, like, this very simple experiment where we basically created some form of a slime. You hear well, slime oracle, where the AI is preparing regulation for itself. And it has a very simple, like, a prompt. Basically, the prompt is that it's some form of an oracle, and it's using traditional folk wisdom, global traditional folk proverbs to give us some rules for the AI, how AI should behave. And, basically, I hope you will hear the sound. I did manage. I did say that it needs to speak, so let's see. Okay. Now the issue is I see the issue is that it's opening a website, but I'll keep it. Let's see what happens. Maybe I need to share the whole screen.

Speaker 3: Don't throw away the old bucket until you know whether the new one holds water. Swedish proverb regulation. Ensure all AI updates are reversible until the new version proves it can dance a tango without stepping on toes.

Speaker 2: I don't know. Did you hear it? Yeah. Okay. Cool. And it's basically this. It takes, like, a traditional proverb and just makes it into, like, some funny statement. And I'll just move now to my presentation to explain why did I do this and how what does it have to do with governance? View slideshow. So now we'll have it. So this is it. It has a weird name, this whole presentation. But this is what I basically showed. Something we started doing, like, last year. You see the slime, and the slime is basically a button that triggers the Python script to give you, like, some beautiful idea from this regulatory oracle. Question is, here is this is another one I actually liked. It says that if an AI chatbot says something stupid, it shall remain silent for two minutes or pay a fine of â‚¬2. So I think some of these, like, recommendations for AI regulation it was giving were pretty useful. But why did why did I do it for the first time? The reason was basically to learn, to imagine some different type of regulation, and my claim today will be that it also serves this purpose of catharsis. It's like a funny moment. It's like some emotionally emotional discharge that happens. So what did we learn through this? It's my buddies that I'll introduce in a minute. What did we learn? We we learn what a system and user is in terms of, these, like, prompts and the ways you communicate with the model. And, of course, this introduces the idea of an agent. What an that actually all communication with this new beast called large language models, AI, machine learning applications, whatever we will call them right now, is basically some form of a role playing. And this got me excited. Like, these, like, funny statements you're able to tease out, and everyone that was, like, playing with this probably has, like, some similar funny prompts and examples. So it looks funny. It was about finding out also how to give a body to the models through the slime. The slime was actually physical interaction with the model. So it's something very pleasant about it. So where this project led is to something that looked a bit more useful, and that's maybe the pro project Coraline saw. I'm not certain. It's a project which we published. I'll send you the link to the presentation because there will be some things I will not manage to cover. Basically, it was the first experience. You can give a role you can give a role of a stakeholder, for example, and give a task, of course. What do you want the model to do with a piece of text? So what we did with my group of, colleagues and, friends, basically, we used an agent, supporting open source model agenda to rewrite the EU AI act, like a year ago when there was this very scary period that they wanted to suppress open source models, which is actually something in regulation that bothers me personally as we see now with California and this, like, a new proposal is going in that direction too. And, and, it was surprising. So, you know, we told it, you're we did it on CHEDGPT. We didn't use any Python script or anything, but we kind of introduced passages from the EU AI Ag, which we wanted first to identify, the agenda and rewrite these passages to fit the agenda of someone who is into open science, open culture, blah blah, like, the good guy. And it was quite surprising that he did a good job in two ways. First of all, it it gave the right format to the feedback you need to give when there are these consultation processes with proposing the regulation, but also it identified some interesting ideas how such community level, like, oversight of a model could look like. So here it proposes it identified passages that are a bit unclear, ambiguous, and then it's, like, told us how with our agenda of a open source stakeholder, we can rewrite the regulation. So in some strange way, it democratizes the it enables public engagement in the regulatory processes, and I decided, okay. Let's go full way into that direction. And I I started to explore more of these agent based interactions, and then it of course, once you create one agent, you want to create more agents. And here is, like, some recent project, actually, that I did with the with the new collaborator, Lorenzo Caballon, who is a SciencePA student. And we went together to this really fantastic governance course by the AI safety. I forgot what is their names. They do this twelve week course. And as part of their course, this was our project. We used the AI agent to agent simulations to simulate an influence campaign on hypersonic weapons and also how such evil bot will interact with we created the persona of a woman, like a housewife from Napoli, and it it was a lot of fun. The model at one point starts speaking in Italian. Of course, there are some surprising and fun parts, but also gave us some, useful feedback in terms of how even a complete amateur nowadays can do, like, an evil influence campaign online and create a panic or push some agenda. So these are examples of projects I work on. So now what what is this for research? Since I'm a I'm an academic. I describe it as research through design that is focused on policy and governance. It's part of a book which I've wrote, which explains it in a more much much more complex context of other technologies and some long term projects I have. The book also has a bot, which you can check and discuss. Sometimes it's surprising even for me what it says. But if I could summarize this approach to research or this approach to governance also is from the more common research agenda where you don't want to create a theory without a model for prediction because that's the golden standard of a good theory. My approach is more like I don't use or create theory without the prototype and not because I want to predict something, but because I want to support participation. So why is this participation important for me? First of all, it's it's some form of agency over the future that I'm interested in, and I don't care if it's AI blockchain or other technology, but for me the technology prototyping is a way we decide about the future. So I do want that process to be a bit more democratic and open, of course. Actually, I don't want this guy. And, then, in terms of something I do, which is science, technology, and society studies, we are really interested how we can place the work on the ethics, regulation, values, and politics early in the r and d phase. So it doesn't come after someone develops something in the lab. So that was basically my focus. And now what this participation mean, I I will show it on this. On one side, you see my official academic career, which may it's not so impressive because I'm still on on some junior position, but I don't care. I always did what I liked. That explains it. But on this side here, and that's the part I'm very proud of, I was always member of different hacker spaces, citizen science, open science communities. A lot of my research is about Indonesian open science and citizen science. So I was always interested in this, like, in in the form of public engagements that happens outside the official structure. And maybe, like, that explains why the participation is also an important part. So the the simulation side will show and because I'm a philosopher, you'll see also my philosophical agenda in this is connected to a work we did in the last two years, basically, with the Tel Aviv Makerspace where I hang out often. And then at Bariolan University where I run something called design and policy lab. And, also, I have collaborators from Czech Republic because I'm originally from Czech Republic. We did some workshops, some community bottom up exploring this idea of bottom up community regulation, and I kinda settled with this team exploratory sandbox be because it is like sandbox is is a policy tool everyone is obsessing over. It's part of the EU AI act and so on. So this is basically my agenda. I want to support some form of a public agency over the future, and I claim that the means to do it is this experience of catharsis, of this I call it the surprise of, you know, facing some fears or facing some impossible situation, some extremes, and, kinda dealing with that and finding some resolution over it. It's a very theatrical experience, of course, but I think that's what the models enable. So it's about this making future together over prototypes to figure out the regulations, divisions of power, oversight, public good, adaptation by experiencing the extremes, by experiencing the conflict also, by experiencing some unsettling things. So and and it's part of this negotiating mutual understanding without demanding mutual goals. So it's not about finding some final unity. It's more about settling on something, experimenting in May experiments in making collectives also, which is a text I really like by Balmer and Calvert, which are scholars working a lot with synthetic biology. So what are these experiments we did? Of course, every hackerspace or many hackerspaces want to have their own stable diffusion server to do, like, porn. I'm kidding. To kinda have a free range of prompts and decide how they will work with this infrastructure. I also work with some government bodies because in Czech Republic, I'm the alternate national representative appointed by the Czech Ministry of it has a long name. It's a bit meaningless. I enjoy working with the people from the government, but they are a bit slow, honestly. So I'm not certain I should even put this. But, yeah, we are trying to create some form of a exploratory sandbox that will be much more public and open than the sandboxes that are proposed. And then what I want to focus today on are these simulations between agents, and I use it for all kinds of purposes like mock judicial trials, AI future deliberation. I think that's what I'll show today. Influence campaigns. I have this new thing about gossip, so I make agents and gossip together just to understand how awkward they are or where it is. And what's the present my original idea of the presentation was this, to explain why I'm doing this. What do I see so troubling about the AI regulations that we need is more engaging public engagement in these forms of these forms of different approach regulation. So I'll explain. So this will be the philosophical part. Maybe I'll skip it quickly, but I want you to be aware that I have this larger agenda in this. Maybe more interesting for us are these present challenges challenges I see in policy, and why do I think experimental governance rather than risk based or commanding control based governance is a better approach. And then I'll show how I imagine such sandbox, and I'll demo at the end this agent to agent interactions. So why do I do this research? I'm really upset maybe or, like, I'm surprised that there is this, like, a persistent idea that we need to automatize governance, that the better governance is something that will happen automatically in a sense. So all these ideas of smart contracts, consensus mechanisms, DAOs, or what Zuboff calls AM contracts. It's not that I want to say that's exactly Zuboff's AM contracts, but what I want to point out is that these ideas that, we will create a system in which it will not be possible to cheat is is this idea of something that will happen by design, by compulsion. Hasselhoff shows it, and she uses Las Vegas as an example of such design. So what I want to say for the beginning is it's not actually it's not a new idea. It's actually a very old idea. So basically, our idea of governance and contracts was it was always it always had this tendency of ending up in some form of a machine technology, technical, something nonhuman, basically. So this is from the, Hobbes Leviathan, all the interesting metaphors he uses to explain what is good governance or what is a contract that will make society, like, peaceful and not kill each other. And in this, I'm inspired by by these famous political theories from the thirties, Carl Schmitt, who interpreted Hopes Leviathan as as exactly this idea of some form of a of a machine, of a mechanism, of an algorithmic governance. Basically, he uses even Campanella's he describes Campanella's vision of a sun state where it's a big ship without a rudder and sail, but driven by a mechanism that is commanded and guided by the possessor of absolute authority. So there is this idea of this nonhuman force that will make the society right in some way. So in Hobbes, the traditional way he describes this is that it's not the it is authority, not truth, which makes the law that Schmidt interprets. Basically, nothing here is true. Nothing here has some ideals or doubts. Everything is a command. So it's usually, people interpret this that it's about might over right. That if you have a power, you decide basically what is the law and right, and you have this control over the future. So I'm a bit different in this. I think it's not this. I think it's something even deeper, and this comes to the idea of Civitas Dei that I mentioned, which is fascinating idea in in Saint Augustinus. It's one of the most important ideas investor and, you know, culture, and we keep all our ideas of Utopia inspired by that, and, it's basically a regional idea of governance by design. You can put it this way, or disruptive innovations that solves our human problems. It's connected. This is the type of terms that Augustinus uses when he talks about this future world, this eschatological, better, city of God. It's it's definitely future. It's this futurum seculum, this future age. It's some new heaven and world. And the interesting thing, he does use the word design, which is novum consilium, actually, it's a world word in Latin. And how does he what does he think of this world? And this is something that always fascinates me about his description. It is a it is a state. It is a world in which where no one is able to sin. So this idea of no one is able to sin, not no one is able to die, of course, and all these other things he imagines, it's, I think, adds a base actually, and it's more important that this idea that might makes right. And it is idea that goes from that Saint Augustine's Civitas dei text all the way to this trustless, whatever consensus mechanism technologies. Basically, in technology, we embodied this old idea and old fascination we have with automation. So here is the the one of the interpretation by Saint Bernard, a guy who is at the beginning of inquisition. Basically, he says the highest form of freedom is not being able to sin. So it is exactly what DAO is about, consensus mechanism. Or here is, like, the poetic version of this from TC Eliot. They men constantly try to escape from darkness outside and within by dreaming of systems so perfect that no one will need to be good. So this is, I think, at the core of my problem with governance today and why I'm for participation. I find this some was kinda ironic that our ideal of governance is that it will not be based on our decisions, that it will kinda be prearranged and pregiven by some automated systems that we don't even know who exactly designs, but it's someone with good intentions. So this is by Jonathan Sumption, a present, like, lawyer and a very important theories to flow and historian who has this, like, a nice way of putting it that what characterizes our present is that we have a lot of rights, but we are fascinated with the idea of curbing individual choices. And this he calls the supreme irony of modern life, that we want something that will dictate us how to behave, what is the perfect state, and what you know, think of COVID when you had the model that tells you exactly what to do, and many of us were troubled by that for all kinds of reasons. And I think it's a version of what Weber calls biopratic domination, or we can call it a technological domination in terms of the of these, like, new tools. It's a domination through knowledge, evidence based policy, synthetic data, of course, all these, like, models that claim they will make these digital doubles and find out. And and it is a form of some extreme control, actually, that annoys me about this. And, again, it is an old problem. We can find it back in the eighteenth century where this French liberal, Andre Morellet, was this was quarreling with the economies. They were called physiocrats who also wanted evidence based policy and governance. And what he tells them, your your belief in power of evidence is actually apolitical, and it's trying to destroy the balance of power, the division of power, and these, like, governance mechanisms we have in society that create, like, a democ define democratic society. And by this thing you see, I am a classic liberal. Like, we still exist, and and that's why my political agenda in this sense is very old fashioned in some way. So where do I see the problem today in the present? It is in these type of proposals and ideas such as this, like, California legislature, this new AI bill, SB whatever, that is trying to set up this AI operation agency. I forgot the name of it. That that will make our model safe, but in the hands of the big corporations because it makes it's exactly what was the problem with the EU AI act when we did this, like, agent rewriting of the act. It is it makes small developers responsible for how their tool will be used. Of course, there is a reason to have to demand some responsibility. But to to make the developers absolutely responsible for the model or demand from them very expensive ways to do assessment and all these, like, bureaucratic mechanism is to kinda curb the the development and also to create some form of a regulatory capture, basically, by the big players. In the EU, we have a similar idea with this EU AI office. It's not so extreme as the one in California, but what is the problem with these things? First of all, if, they're soft like the EU AI act, they ended they may end up like the GDPR. This is called risk based approach to, governance. And as you can see with the GDPR, this is like their results from last year, so I don't know how it's now. But from eight hundred cases filled in the last five years in 02/2023, fifty nine had some success, eighty six are waiting for decisions, and they found 60 procedural issue. What this means is everyone interprets the, the regulation in their own way, and it creates, like, this mess. Basically, it's better maybe not to have a regulation than a bad regulation that is not efficient. It's it's like it can be interpreted in many ways. And then I think the other approach, which is more like, let's create an FDA for algorithms, which is this California approach right now, is an approach we know very well that is failing also. It is the famous case, of course, the Sackler family and all this opioid crisis. So the present pharma type of regulations that depends on this command and control approach of this agency that is kind of following and supposed to do oversight but gets corrupted, it is a problem. Because too much problem too much power of whoever you give it creates the issue. So what is my proposal to get out of this approaches? I believe in this experimental governance where I imagine there should be sandboxes or there should be, like, a test periods where the tool, whether it's a blockchain, AI, whatever, can be continuously maybe, like, tested and people, different stakeholders can give feedback based on the negative experiences or some issues they see. So there should be some process of deliberation and negotiation. So there shouldn't and and this is what I claim, basically, we need a space to politicize the algorithms, the technologies, not claim, hey. We we have values. We don't even say which values, and we put it in the algorithm. Everything will be solved. So I I see a reason, and that's why I'm interested in the simulation as this open agonistic environment for experimenting and so on. What I hope to achieve by that is this, to take action, negotiate good enough rules without necessarily agreeing this is absolute good or these are some core beliefs or core identity we have to upheld. So it's a very so it's a very, like, pragmatic approach. Here again, some inspiration how Jonathan Sumption puts it. I like this. It's it's a good rule. It's a or good regulation is a regulation where minority will prefer, but the broadest possible range of people can live with. So it's not a win win, but it's not lose lose for polarized society. It is some middle ground, basically. And it is something in the STS field, science technology studies field where I'm active. It is something we describe or use in concept from anthropology as trading zone or border crossing, which is often the model how science communities come up with, like, interesting discoveries or ideas. It is basically coordination and exchange of knowledge and resources between the similar and even antagonistic actors in short. But it is something that will happen in a pirate colony where people would meet, the governance wasn't clear. Given the language, they didn't use the same language. So how does this approach looks in practice? So I use this I needed an environment which will let people that are maybe not technologically savvy generate like, create their agents that represents them in some simulation or in some situation. So what I use is this GPT team fork, which is on on of the line chain people the line chain framework is the framework where you can basically build application that use different models. So it's also good for research because you can test it on different models. So it's a long chain framework application built by their team called GPT team. And I made a fork that kinda helps my agenda, which is more like discussion. Like, the simulation they do is more like they're in different it's like a little city or office they have, and they want to see whether how these agents behave. For me, it was more important that the agents talk together. So it's on GitHub, of course. This is the original project. So what are these agents? They are part of a world. These are different parts of the code, basically, and what what are the functions and what happens there. They have a bio, directives, memories, and plans. This is classic thing for every agent. What the memories are is maybe the interesting part because these are embeddings on top of the model, while the the directives are basically how they define their action. So in the memory so maybe this is the moment I'll just show how it looks like. So I need to do another just so let's go here. Maybe I do screen. Okay. I'm on screen, and I need to open a different application. I did I did rehearse, so it should work. On on GitHub, I made a description for everyone to run this application in this Python script. And I'm a philosopher. If I can do it, I believe many people will be able to do it. So the so so first of all, I will not go through all the code. I'll just go through the part which is easy to under understand and where I involve engaged stakeholders. So I do these workshops where I ask people, okay, what is an issue you want to resolve? For example, future of AI. This is something we worked on in Amsterdam in July, and I divide the people into groups that will define their stakeholder in that discussion. So then there is something here is like a location. It's a bit small screen. So the location here is like a place where people discuss, place for public discussions, where citizen confront each other, blah blah. And here are the agents. So in that particular case, there were three agents. Since it's about the future of AI let me word track. So it's a future of AI, so we went for some position. One extreme position was this, like, a Luddite or anarchist called Gaia. And you see the agent. It has a name. It has a private and public bio. Why? Because sometimes it creates an interesting contrast. It gives the agents a bit more some, like, death or ambiguity. So, you know, the private bio can be a secret agenda to agents. So here we made her super simple. She simply anarchists who died. She wants to destroy all AI. And then the directives are important because they define the actions and the priorities based on the script there when she starts communicating. She has an initial plan. And the stop conditions so far, I didn't completely figure out how that works. It never happens. The agents, like, kinda continue forever until there is some glitch and problem with the code and so on. Then, here we have Gaius and we have Sam. Of course, we made him a bit of a psychopath who wants a monopoly and re regulatory capture. So that was a bit evil. And the third person is, like, some form of an AI mystic called Arturia. So the whole prompts were more complex, but I made it now simple for our purpose. And what happens is I basically I will first maybe I'll try to reset the world because I'm not certain what happened in the previous and then you have a simple command where you basically run the world defined by these agents, and I hope you'll see what happens. So it started you see, it started generating, and soon it will open basically in the browser a page so you can see how these agents discuss together. I'm running a bit out of time. I just want you to kinda get because the problem will be clear when once you you see now the how they're checking together. So they're observing. There is some action in progress where the action in progress is defined basically on the directive. So Arcturia, our, mystic, she wants to make everyone excited about the possibility of nonhuman oops. Disappeared. You see? Of nonhuman future, and she's all about mother earth embodying this AI consciousness while Gaia is trying to destroy everything. She tells Arturia, your vision is a dangerous one, delusion. AI technology is a parasite feeding off the lifeblood of our planet. You know? It's energy consumption and all this. While Sama is trying to convince everyone that he should be in charge of the regulation and the development of the technology. And now they all let them chat forever, and I'll explain And now they all let them chat forever, and I'll explain why what what was the plan and what actually is the problem, and I'll finish with that. So the plan was basically to let stakeholders define their agents, and then by watching what is happening in the in the in the simulation to the hypothesis is people will experience some form of catharsis. They will see the dead ends of the discussion, or they'll come up with some new ideas. It will be a bit like a theater experience. But, of course, I don't still have a good research design. My question now is what to do with these locks? Of course, one thing to do with the locks is what we do with all agent to agent simulations or some analysis. You can use Chegg JPT, which I did. It's kinda interesting. This is from a different, a different simulation where I made the doubles of ourselves in the discussion. It was about AI governance some couple of months ago, and I could kinda see who who from which agent was speaking most of the time and then ask questions why. We were also looking into how, our discussion differs from the simulated discussion. I cannot say why. It was kinda interesting. I would use the logs, then say, do a summary of the logs, and then make some video or make images. The video is very cringey. I can show if you will see. I did audio play. That was maybe more interesting. So the logs, you hear them one after another, so it looks a bit like a dialogue. So you still have a lapse, and I'm still playing with this more like audio representation. So, yeah, this is where I am. I'm kinda claiming this is some form of a quasi public synthetic space in which we can preserve agency by discussing, deliberating about issues. I'm also saying this this can be a public sandbox, not like the sandbox the EU AI act and similar regulations want to make, but much more democratic open space. Yeah. It's about connecting education research independent oversight. I see it also as a form of a ritual, which I'm kinda pure becoming curious about because it's not clear. Is it an object? Is it a person? It's a bit like channeling. And this is my summary. While most of AI regulation and governance solve all this issue of trust, I'm not interested in trust. I don't trust any technology. I don't trust any government. I don't I don't need, like, a strong trust. But what I do need is some form of an engage engagement and agency, ability, if I don't like something, to be able to say something or collaborate with other people and change it. So here I see that regulation should use maybe more prompts or beyond that level of customizing the output, interacting with the models. And what I'm looking for is this issue of catharsis or experience of catharsis into these probes of power stakes interest. So I see this like an environment that is good for politicizing prompts, using it as a tool of public imagination. I don't mind machine hallucination. It can be inspiring, actually. So about I see it as a theater, and that's my presentation. Sorry. It was a bit longer than I wanted.

Speaker 1: No. It was awesome. Thank you so much, Denisa. Wow. What what an awesome presentation, and I loved someone in the comments. I know you probably didn't get to see the comments popping off during your presentation, but a lot of cool reflections as you were going along. Ian says this is a fascinating long view on some of the ideas that circulate in and around Medigob. So you're definitely in the right space, folks who have been thinking about experiments, play and, yeah, simulations, all that type of stuff. We a lot of us have done, yeah, some cool experimenting. So I'm excited for this discussion. I have a stack that was loosely based off of the chat. I typed stack first, so I'm gonna steal the first slot. But, yeah, if I got this wrong, if you posted something and I missed it, just just let me know, Steve. You'll be added to the bottom. I just saw your hand go up. So, yeah, let's let's get into it with the fifteen minutes we have left. I guess, Denisa, for me yeah. I've done Sent and I worked on a project called d 20 governance that was in Discord, and we used, yeah, like, chat g p t, but we sort of created an environment in Discord where communities can play around with different governance environments and, like, world build and, playing with governance and using different, kind of communication environments to alter what you might think of as, like, power in that space, like, the ability to you know, if if, for example, you, in our in our world, like, if you were to, type something that was, like, too extreme of emotions, for example, like, it would, like, exacerbate it even more or could censor it if you like. You know, we're too extreme with your emotions, or we kind of just created all these alternative environments. And I I think, like, something that we struggled with doing that research and in that project was kind of, like, attaching or kind of, like, then thinking back to sort of, you know, as as fun as these environments are and as insightful and useful as they can become, like, how do we then, like, I guess, deliberately connect that to, like, use cases of AI in the world and show, like, yeah, through experience, but still, like, pointing to a a a way that we, like, propose a change of something that we're currently experiencing. So I'm curious if there is sort of a concrete, like, policy outcome that, like, does sort of translate across. Like, of course, it's all about the game, the process, the catharsis, like you said, and I totally agree with you. I guess I'm curious, like, what do you how do you describe that, like, channel of communication with, the the, like, policy mainstream, like, policy conversation happening right now?

Speaker 2: So you probably know, but public consultation should be the place for engaging with the regulators and kinda enabling the voice of the public. But it doesn't work, of course. We all know and have experience with that. Why is it? I I I mean, I wish I knew the response. Like, even as someone who kinda entered the government with this idea, maybe I can change something if I join the professionals. They're so busy. They're not evil. They're just very busy, and it's it's the same problem as everywhere because these people are evaluated by something. It's just it's really difficult to go to bridge between different worlds where people are evaluated by different outputs and they're in their little worlds. And, of course, I think you're kinda all victims then of something. So I'm maybe more I'm pessimistic. I kinda think maybe the cool thing where I feel some limited success was actually with these agents, you can rewrite the regulations so you can kinda adjust it to the format the regulator on the other side will take it seriously. Because if you just say, I don't mind this regulation, blah blah blah, they will not listen. But if you put it in that format they need in order to take it seriously, maybe it will work. So where I would like to see it is maybe in this in this professional public consultations moments to enter with the agent, rewrite it, and it's a bit bit of a stretch. But, yeah, I don't know.

Speaker 1: Cool. Yeah. And, also, wanted to just add, folks were asking if you're willing to share your presentation links.

Speaker 2: And Yes. I'll I'll share it now. Just copy the link.

Speaker 1: Thank you so much. Yeah. And, GitHub links as well, please. Cool. Yes. Next up, we'll pass to Yanis. If you had I saw you had a couple questions there, so the floor is yours.

Speaker 4: Yeah. Thank you, Lord. Thank you, Lord. It's a great presentation and great discussion. My my question is, right now, are you gonna be able to avoid kind of, like, an infinite analysis paralysis kind of situation, which, of course, you have both on the on the semantic level, but also on the on the on the syntactic as well. So ChargeGPD, I don't think it will give you a a layout. And I was also curious if more or less than the ChargeGPD is do you plan to cover actual humans having similar discourses as the the automata you are creating? And can we correlate can can can you compare that this course threads and depths that the AI generated discourse and the human generated.

Speaker 2: I I hope I didn't hear you very well, Yanis, but I I understood you're asking me about some form of a comparison of the outputs of the simulation and the real deliberation processes.

Speaker 4: Yes. That was Hector O'Leary question. The original question is Skyger gonna avoid this analysis paralysis? That's when we discuss regulation. Right. That that was the original one. Yes. Thank you.

Speaker 2: So I was working with GPT four mainly. I'm kinda trying to play now and see how it works with cloth and, like, something about and this is what people do with these simulations. They kinda try to use different models or use one against the other. I'm actually going to a professional conference with agent to agent simulators to see what these guys are doing, to kinda see what can be done with these locks. I'm like, you know, that's why this presentation is so open, and, I'm honest. Like, I don't for me, it's fun to do it. I'm actually, what I think will happen is I'm probably we'll go more into the artistic direction. I want to see how this can work as some form of a public performance where people are engaged through creating the prompts, letting the agents fight other agents. So something more more fun than I'm skeptical it will change regulation. But maybe maybe it's a form of a protest and disobedience. So so there, I see it more.

Speaker 1: Cool. Awesome. Next up, we have Josh. You also had a couple questions. Floor is yours.

Speaker 5: I think it's a we'll just briefly come up with our on the camera and say hi. But the I guess, reconnections, I'm not sure. One, we have a project in Medeco called COI or, I believe, knowledge objects infrastructure. It is effectively a LMM agent that is active in the Medigot Slack. And I would just say it will be really quite fun, I think, to try out some of these experiments inside our Slack. If that's something you'd be interested in, we're willing to share some of the code with and help us deploy. I think that'd be quite fun as I think we've been thinking about how to enable the governance of these agents or or, you know, dispense the governance of those agents. It'd be kind of fun to to do that directly as part of that deployment. Second thing is I know you were down on regulation, but one of the things that Medigap organizes is this larger movement around public AI, which is basically an attempt to get a bunch of governments to build their own AI models. More than that, it's kinda like this idea around AI as a kind of public infrastructure. Maybe we can build an AI like we build highways, like we build public utilities, like we build the Internet. No way. Right? This is a long going process, but I just wanna point that out. As part of this, actually, one of the things one of the folks who have been approaching this from a more artistic side is a group called Serpentine. They're based in UK. They have a gallery space. And they actually just had a series of showings around this idea of public AI. I just mentioned that because it might be a group you'll be interested in getting in touch with. I'll just put it there. There's a few things to follow-up. Okay? You might be offended.

Speaker 2: Fantastic. You all the all these all these activities sound something is happening. We are, you know, we are all excited about these agents. So, yeah, it's worth of experimenting and kind of mapping the different practices and seeing what where we can collaborate, I would love to. I'm interested in general in anything that that doesn't take governance as something or, you know, their values and we just have to embody it in the infrastructure. Some things that's much more recursive or much more reflective that enables more engagement and has these good enough goals.

Speaker 1: Cool. Yeah. I'll add, like, Sendh and I are current community leads at Medigov. And so after this seminar, it would be awesome to continue the discussion in Slack and discuss, like, maybe a potential sandbox that we can create with you, Denisa, using the koi experiments that we're doing already in Slack. I feel like there could be a lot of potential overlap. And, also, I'm taking notes so I can make a list of some of these partnerships, and and I love to map different projects and, out there in the space and and facilitate introductions and stuff. So I will definitely

Speaker 2: Yeah. Just just to make one thing clear, like, the word regulatory sandbox, I'm using because it is something the regulators are excited about. For them, this is like regulatory innovation. So they want to see some form of a stakeholder engagement, but the way they understand it is, we'll just invite someone from the government and the people from the company or whatever. It's a technology and maybe some other important people to but they, like, tend to keep it close. And what what what I'm kind of pushing is, let's make it a bit more public. Why there are not people with from the NGOs? Why there are not, like, motivated citizens that see what you're experimenting and working on? So you get a bit of more of that type of public output. So the sandbox is just a term they use, but I think we should appropriate it in case we want some better governance, basically.

Speaker 1: Totally. Alright. We have only six minutes left. Ian, you posted a question. Please, the floor is yours.

Speaker 5: Did I post a question?

Speaker 6: Or I feel like I just commented. But I I would say maybe more of a comment, but I'm very interested in this idea of, like, a public, you know, kind of theater or interaction or sort of a performance. And it makes me think, like, I went to Defcon, last year, you know, the big cybersecurity conference in Vegas. And the AI village there had this, like, red team challenge that was coordinated with the Office of Science and Technology Policy, which had written the published the, like, blueprint for an AI bill of rights in The US, but also then helped develop the executive order on AI. And that was this like, it was a game. Right? But it was a game that was very kind of set by certain normative rules and the ideas of what it meant to, like, try to hack or jailbreak or red team these things that is very different, I think, than the kind of play that you're imagining. So I wonder what, like, a large, you know, kind of public challenge or public engagement would look like with these kind of agents. I think particularly given some of the perspectives that they're espousing that, like, might be things that you might see filtered out, from a more, like, you know, government sponsored or kind of professional, approach to these things.

Speaker 2: This I would love to see what's if there is any documentation which I doubt because it's DevCore. But, yeah, like, the idea we were working with Lorenzo is some form of we call it AI purgatory, like a place where you can be exposed to the evil bots and see what are your vulnerabilities. Like, you know, like, for some form of an arena where some fight happens or, you know, you're upset about whatever, Russian bots or any other issue you have with the world. So something like a fun environment where you can go. I think also Val kinda set that experiment since the beginning she described has these qualities where you get exposed and you see your vulnerabilities. You let your agent, of course, fight other agents and I don't know. Some this is the idea.

Speaker 5: Mhmm. That's awesome.

Speaker 1: Yeah. I sent the link. I'm actually connected to the group, the human intelligence group. I think they were the ones that organized the Defcon red teaming, and I went to their follow-up challenge here in New York a few weeks ago. They're doing a lot of, like, bounty hunt type of stuff. And, yeah, I definitely think that's a great, another organization to add to our web of potential partnership collaborations. They're doing really cool stuff there.

Speaker 2: Yeah. Cool. And I'm really curious about theater or ritual as format where it's just, like, form of, you know, know, being experiencing some extreme states and where where you go to build some form of a public decision or public, you know, you you you're forced to see tragedy or you're dealing with some you create as a some communal type of experience that I think creates bonds. So I don't know whether that's possible over these simulations, but I still don't didn't test that, but that's where I'm going.

Speaker 1: Totally. Alright, Steve. Can you squeeze your question into a quick two minutes?

Speaker 7: No. Not at all. But I will say that it I mean, yeah, it was just it was just all too much. But I I think that, I do have a way to concretize a lot of your approach and that I need help with a prototype to do exactly that.

Speaker 1: Yeah. Steve, please send along your link different stuff you're working on, but I would love to add it to our Slack thread after as we kind of start to plan next steps. Because, yeah, Denisa, like I mentioned, I feel like so much of us have all these different you know, we're in these different rabbit holes and stuff, so have lots of kind of similar but different projects or stuff that we've come across. So it's so nice to welcome you to the space. If you haven't I know you're actually in the Medigov Slack already, so we'll continue conversation in the thread in the Medigov dash seminar channel. Sent posted in the chat that they are sorry. Senti. Senti posted in the chat that they're willing to hold space for another ten, twenty minutes if anyone does wanna stay in and continue discussion. Unfortunately, I'm not able to stay, so I will have to head out. But if you're interested, please stick around. I'll open up the channel in MediGov in the Slack, though, to continue async. But let's unmute and give our presenter a round of applause. Thank you so much, Anissa, for joining us. What an awesome presentation. Everyone unmute and go for it.

Speaker 6: Fantastic.

Speaker 1: Cool. Alright. I'm gonna make Senti host and head out, but, again, feel free to stay on. Thank you all.