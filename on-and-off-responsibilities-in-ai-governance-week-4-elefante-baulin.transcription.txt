Speaker 1: Hello, and welcome, everybody. Welcome to another Medigap seminar. Today is 02/21/2024. I am Seth, Hostin. I'm the community lead and manager at Medigov. And, today is the fourth and final week of a four week seminar series that we've been experimenting with here at MediGov and in collaboration with Responsible AI Institute on and off responsibilities in AI governance. It is co hosted by myself and my collaborator, Manon. And we're gonna we're trying to just, like, jump into the sessions as quick as possible. But just to kind of give you a little context for what's gonna happen today, we're gonna have two guest presentations from participants of the Medigov community, who will be talking about different aspects or picking up themes that we were discussing in previous seminars, and giving us some five minute short talks, or lightning talks, and then breaking out into breakout rooms and leading some discussions for ten minutes. So we'll have the short talks, and then we'll open up the breakout rooms. And then you, as people who are attending, can choose which room you wanna go to and which conversation you wanna have. I also wanna just take a moment to point out that the seminar is that these presentations rather are supported with a $200 honorarium, which has been made possible by the Medigov membership program. The membership program, to date has been a group of people who are pulling together, contribution resources, through into MediGov in order to try collective self governance. And this is one of those decisions that we made to support the conviviality of our community. If you find this kind of work important or valuable, I encourage you to consider becoming a member. We have contributions starting from $20 and $50. And there are links in the agenda for you to check out if that is interesting to you. So, our present our presenters are gonna be, Val Elefante, and, we also are gonna have Vladimir, Bulin. Please, forgive me. I don't think I've said that name out loud before, so if I mispronounced it, hopefully, Vladimir can correct me, when they start their presentation. Val will be presenting on reliable community data annotation from a responsible AI, and Vladimir will be speaking about ThinQ, a data centric knowledge sharing project. We'll start with Vowel for five minutes, and then we'll move to Vladimir. There's more information about both Vow and Vladimir and the respective projects in the agenda. So with that, I'm gonna go to I'm gonna pass the floor to Vow. There'll be some timers at the bottom, to help everyone keep track of time. And, I'm really excited to start, the next letter.

Speaker 2: Great. Thank you so much, Zen. And hi, everyone. It's nice to meet y'all if we haven't met before. My name is Val, and I am going to share my screen. Awesome. Let me know that you can see this, though, actually, I cannot see you. So please speak that you

Speaker 3: can good.

Speaker 1: Okay. Yes. I can see. Amazing.

Speaker 2: Amazing. Awesome. So, yeah, thank you all so much. My name is Val. I am a researcher, and I have been part of the MediGov community for, like, two years now, working on various projects within MediGov and beyond. I'm generally interested in AI stuff and community governance stuff. So this presentation is called community data annotation for more responsible AI, and it's a project I've been working on outside of Medigov called Reliable. But I've learned a lot from Medigov that I've applied to the project, which you'll see. So I work with Annie Brown, who's an AI researcher, as well as a whole host of other amazing people from around the world truly. And this was our our main problem statement that got us started in this work. So over 73% of LGBTQ plus content online is flagged as inappropriate. We realized this back in twenty eighteen ish and have been obsessed with solving this problem ever since. So back in 2019, it was kind of a big moment. FOSTA SESTA was passed, which is a a backdoor on section two thirty. It made content moderation a lot worse for our community of queer LGBTQ sex workers, erotic artists, and just generally, like, marginalized communities, BIPOC folks, disabled creators. You know, here are a host of headlines that kind of capture the problem we were facing on mainstream social media. And so we built a better platform. We built, an alternative. It's a web app. We're not allowed in the App Store, because we allow nudity. But it was a experiment in creating a safe place online for these communities, that also allowed for freer expression. And we learned a lot from that experiment. We built the platform, through a series of co design sessions. So I wanted to include that because co design is a really crucial, aspect of our work and, practice ever since we started. So, I think that's a key piece to understanding, you know, the responsibility aspect of how this all fits in with AI. But, basically, from Lipz, we we Lipz being the social media, we built a content moderation software solution that basically can allow for more nuance by giving powers more control over how their content is being classified. So it's a and it's an API, and it fits into any platform where there are community kind of user generated content, data. And it really you know, the the key is that, community data labeling aspect. So, that has a wide range of repercussions, and I'm excited to talk with you all about, you know, how you can see this kind of core concept fitting into your work. But at the core, it is about giving users back power over how their content is being labeled and doing kind of collective data labeling as well. So the effects of our work have been that algorithms can become more accurate and take better into account context and nuance, especially for kind of some, quote, unquote, gray areas or complex topics such as race and sexuality and gender and, disability and all of these kind of topics where, you know, there are, I guess, just algorithms that are blanket on either misunderstanding or misrepresenting or just imbued with bias. So our system has enabled a a kind of more nuanced and advanced approach to solving or reducing bias or creating more accurate and efficient algorithms in as well as a greater sense of agency and trust. So there's, like, inherently more transparency and trust built into a system where users can have more control over how they're being labeled and also trust within a community kind of collectively deciding, oh, we're all going to care about how our data is labeled together and and features, that we've built enable that kind of collective labeling system as well. And then things like regulation compliance. Like, I feel like we're in a moment where regulation is, at least in The US, like, about to come down hard and and around the world, like, you know, more, transparency algorithmic transparency laws and things. So okay. I guess that's my one minute left thing. So we can talk more about regulation. But here's how it kind of looks. You know, there's a tagging system. This is what the user kind of sees. You can add and remove tags based on different classifications that you and your community decide. I won't play the video, but this is the Lipz platform. If you're curious and you like queer art, check out lipz.social. This is how it kind of looks like implemented on a real application. Then there's the platform side, and this is, you know, really flexible. Right now, there's a moderator panel of about or moderator group of about 10 of us who are elected basically, to the community I mean, by the community, we we did a system a few years ago of kind of choosing who is gonna be our moderators, but they're all community members themselves. And so but this is a really flexible system, that gives, you know, experts in the community or community stewards more power to see and approve and and change tags based on how accurate they are in the community. And I'll close out with just kind of the impacts on you know, you can imagine sort of how this has played out, especially around creating a more inclusive environment for women and queer people, as well as thinking about other implications for reducing discrimination bias when it comes to, for example, financial services, like payment processors that don't wanna work with women and like sex workers or queer sex workers and other folks of marginalized identities. So that's my presentation. I have lots of discussion questions for us in my breakout room, so come join me to talk more. And this is a QR code. If you are interested in booking further time to talk about any of this, please scan and, talk with me and Annie Moore. We'd love to hear about what this brings up for you.

Speaker 1: Super. Thank you so much, Val. We're gonna move, straight over to Vladimir. But, everyone, I encourage you if you kind of wanna keep track of your notes or your thinking, feel free to use the collaborative notepad or your own personal notepad in butter. Okay. Vladimir, please. I think perhaps you're muted, but your screen share is starting. Okay. Vladimir, we still can't hear you. I think

Speaker 4: Right now now I'm okay. Yes. My name is Vladimir Balin, and I'm working in Tarragona in Spain. And I will present you our project link, which is now I will start this project. Now you should see the screen. So this is our project link, and this is about decentralized co creation and artificial intelligence pro platform, which is applied basically to many things. And we can we start with the farmers. And this one of the message from this platform that, for example, for the ScienceNow AI make science accessible to everyone through this through the learning platforms. And it is possible now everyone to be a researcher and to go outside of the lab. And this is a what this project is a demonstration how how scientists, farmers, and service providers can be linked by AI in in the actionable intelligence for distributed research. We have a website, so you can check the website and also to see how this participatory research can work. And the main idea is the data centric knowledge sharing, which is going through the feedback loop from the learning experience and through the real data from these fields. This is example here which is connecting human actions and measurable and verifiable states. For example, if this is this project is about improving practices for agriculture and moving towards more sustainable practices and treatments without pesticides. And here, what we when we see, this is a human actions, is a practices and treatments, products with the which are applied, and those are decided by humans in order to improve soil quality, improve production, mitigates disease. And the the the there is a link between those actions and the measurable and verifiable states, which can be measured with sensors, with the weather stations, with laboratory tests. And the the AI is can be built on on this link. So this it it is learning what humans are doing and exploring the parameter space and the the is there real effects on the soil? So there is a real impact in biodiversity in in in in this weather and also in the soil quality, and we see the real impact. And this real impact can be benchmarking the human actions. And so then the machine learning is trained on on this loop. So instead of so this is and if we think about those farmers which are acting independently on on this platform, so they are what they are doing, they are constantly doing different things in their fields, and this is recorded. And then we record it through the book of operations, through the data actions are recorded, and their result is also recorded through sensors. And this can be a a system where the collective intelligence is a through this feedback loop and, additionally, the farmers may also be connected with the knowledge created by other farmers which are in the system. So this and their artificial citizen if there is a system is built on the total information collected from the fields. And this can be a system which is maybe contrasting to what is known now that instead of looking at AI as a pet, which have to be controlled and limited as in instead of focusing on the human needs and is trying to build for humans, This system is built on nature laws laws of physics and biodiversity. So the system is trained on the real data from the fields. So the objective is to improve the biodiversity, soil quality, or the improve real of the climate and instead of focusing on the people. And then this gives a much more robust and and this is flexible system when we don't need to include to worry about human biases. And this works already, but we can show the results. So this is a connection. So this IAI is is here is basically, is a connection with the real consequences of the practices, and they are this is a connection of the sensors' metadata historical records and the the the real production and the real improvement of the environment that can be traced and recorded. And Okay.

Speaker 1: Vladimir, thank you. We'll have to end it there. It's it's a nice, image to end it on. And if you were interested in going deeper into the topic, then they'll be able to join you in the discussion breakout. So Okay. Yes. Thank you both to Val and Vladimir. We're going to go ahead and open up the breakouts now. I'm gonna do that, and it'll probably just take me one moment to do. So bear with me. Okay. So the breakout rooms are opening in five seconds. Val and Vladimir, you'll want to go to your specific rooms.

Speaker 4: Well Yeah. Mhmm.

Speaker 2: Hey.

Speaker 1: Okay. Welcome back, everybody. I it was really nice. I got a chance to go into both rooms and catch a little bit of the discussion. I also saw some comments about kind of the strangeness of the platform. Keep those comments in mind. We're gonna eventually send out a feedback survey. We'd love to know what people think of using this. So at this point, we're going to move on to the next portion of our seminar today, and I'm gonna pass it over to Manal, who will be leading us in the second half here. And Manon, I know I didn't really properly introduce you, so maybe you would like. Maybe a short introduction would also be welcome.

Speaker 5: Sure. And hi, everyone. It's great to see you again. I'm Manon. I'm a research scientist at the Responsible AI Institute and a, fellow at Harvard at the center. And so to wrap up this series, because it's not just, you know, the last twenty minutes of this of today's seminar. It's also the last twenty minutes of our entire series. We wanted to do two two things. First, our collaborative writing sessions that you've experienced a little bit since we started the seminar. And the idea is for us to reflect on two questions. And the questions that I have written in the collaborative documents are about how do we create guidelines for responsible AI at large that remain current. It's a theme that we've discussed a few times over the past few weeks. And one that seems pretty hard, and we often talked about it but never really answered it. And so that was the kind of questions I wanted for us to think about. And the second question is about whether we think that there are very specific type of governance systems, like maybe citizen boards for the, for companies that could be created, you know, kind of like a governance innovation that would allow to have a more responsible use of of data and systems and just being more responsive to the variations and, huge externalities that these technologies made up. So I would open the doc in a second. The idea is that for about ten minutes, you just write what you have in mind, and, we'll spend the last ten minutes reconvening, discussing what we've we've written, and also getting overall, you know, feedback and ideas on the entire seminar. And then we'll be ready to go. Do you have any question so far? Okay. So let me open the shared document. As always, you have two questions. Well, as always, you just have questions written, and the idea is that we can do collaborative writing. Feel free to, you know, merge. If if you see that there are there are two things that are the same, the idea is to have this kind of like, collaborative document that looks like almost a report we could submit, by the end of of the session.

Speaker 1: I might just also add to kind of, kind of, encourage the writing, and kind of making some connections across the seminars. If you have any kind of references to past seminars or even the presentations and discussions that we had at the beginning of today's seminar, it'd be really nice to point, like, try to reference those if you can, so we can kind of make some connections across all of the events that we've hosted.

Speaker 5: We'll be waiting for about two more minutes, so wrap up your thoughts. Okay. Let's I see there are some nice cursor last cursors are running around. Let's let's step back and have a quick look at what others have written, if you're not already done so, to get a sense of of where the group stands on the questions. Okay. I will I will close the the dock in just a second. And let us go back in. Thank you so much for for doing this again. It's every time we lose this sentence, I are just mesmerized by just the quality and the amount of content that that we get in writing altogether. So thanks a lot for for doing this with us. So we'd love to, you know, open this last, eight minutes for the community to discuss some things that you've learned, if you have a reaction on what we've we've done just right now, or anything else that would be really welcomed. A reflection on these past four weeks, some recommendations for us for next time, what we could do better, what we couldn't do better, what was good

Speaker 3: as well, is is what

Speaker 6: I would just say I like the participatory components on this kind of, like, iterative writing and knowledge building. It's really fun to see, you know, sort of as almost like jamming on ideas, but I'm also very curious to see, you know, what is it that will come from that or, like, you know, will these documents become, you know, some sort of artifact? Like, in some ways, they are an artifact. Right? But, like, in what And kind of seeing the process develop. So I'm just hopeful and excited by that.

Speaker 5: Yeah. It's something we'll so I'll I'll go back to them to the documents and, you know, try to write some type of of a synthesis. We we don't have a a, you know, a clear plan already about having a report or something public. But if that was the case, would you would you like to be associated and Okay. Write or to be contacted afterwards? Okay. So we make sure to get everyone who wants to be associated or write or just be mentioned in this to get your your emails and to get the text into your hands before we go through.

Speaker 6: Awesome. Thank you.

Speaker 5: In fact, for those who are already here, if you want to do this, can you write your email in the chat? I'd collate them right now so that there will already be a question. And if you had to say, like, I don't know, one thing that you felt you either you learned or a new idea that was formed during the interactions that we had with the group, with the speakers, with the writing, what what is it?

Speaker 3: I think I've learned that there's more participatory I mean, obviously, MediGov, that's kind of the premise of MediGov. And I feel like in in the Web three blockchain land, participatory systems and, like, democratic systems are pretty commonplace. But I think I haven't been as aware of it in AI, and in responsible AI. So, it was exciting. I think I said maybe this on the first call, but it was it's exciting to, like, see there are others, that are interested in this and that are, like, talking about it and doing real work in it too. So, it's like I have a I made more I made friends, which is fun, and, like, new connections. So I'm excited to to continue those dials. I'm actually Nathan, is speaking his event, at the Brooklyn Library is, in two weeks or, like, week and a half. So I'll be there. So if anyone's in New York and is going, I'll be there. We can meet up.

Speaker 6: I'll be there.

Speaker 3: But, yeah, I think yay. Cool. Hey, Ian. I just thought of doing LinkedIn. But I think yeah. It's it's nice. I feel like I didn't I haven't been active in MediGov. I'm in a lot of Slack groups, so I think it can just be overwhelming in a lot of different communities. And so I I like this small structure and, like, kind of the the intensive seminar series I I like. So I recommend more of them. And, like, maybe group projects or, like, collaborative activities. Like, I think the the Google Docs are cool, but then maybe whether that's, like, building a small prototype or, you know, coming up with I love I like cocreating, like, recommendations, open source recommendations or blog posts or things like that. So it's like we can take all the learnings that we have here and share it with others and get more input. And I'm I'm happy to do that also under my, like, official work hat as well for personal and professional work capacity. Thank you, guys. Thank you, Manon and and Sint.

Speaker 5: Thank you for your great participations. And it's good that you met people. And and they are I agree. There are so many of these very cool participation tools that are kind of coming up. And we need to know how to integrate them well also in our workflows. And think yeah. It sounds like really exciting changes in how we do collective intelligence tasks. A touch of different yes. Yeah. And I see your comment. Yeah. And if it to me also, it feels like at school, there are so many things that there are sometimes almost too many things, and you just want to, like, experiment with all of the different features.

Speaker 7: Yes. I I think it's there's something that maybe a suggestion. I was in the first, the second. I missed the third one. The first one we play, it was like for me, it was very different. Like, you know, children learning back to to to do to draw or something like this. Maybe what would be interesting and and and going to what Ariana said, it's to work the method, you know, what we're doing what would be nice, for example, is to have a a small manual or for that that the place, you know, because we are each one learning on our own, and we don't get the same amount, I would say, of benefit in a way. So some people might might get frustrated. I did not. But in in this session, I was able to, you know, to put a question. It would be nice, you know, those that enter, they they they they can talk, they can draw, and those kind of things because it's it's very different from Zoom. You know? Zoom is like a a place aseptic. So you know, you don't have and almost you're getting sleepy there. And here is a contrary, but there there's too much in a way. So we that's something I think. And maybe to do, like I I don't know if it was Yan, have an artifact or a a co production of something at the end.

Speaker 5: Yeah. Thank you very much for the feedback. I think we also had this feeling here. I see the same thing in the chat that we we probably could have done the lending easier when we started the the using using the new tool, and it's great feedback for next time.

Speaker 1: Yeah. Okay. Well, it would actually be really nice to just I mentioned in the chat that our is always feels so short, especially when we have so many interesting things that we can do here with each other. But I just wanna thank you all for coming and participating in this experiment and getting a chance to reflect on it a little bit. And Manan and I also wanna put together a a survey that everyone who attended can fill out. It'll probably take us maybe, like, a week or so to put that together. But afterwards, that'll be a great opportunity to continue thinking together about how we wanna do this kind of collaborative work in this context. But, yes, I also wanna just thank Val and Vladimir for presenting today and leading discussions. It was really, really nice to get to hear some people kind of extending the ideas that we've been exploring. And I hope that in future, formats like this, that continues to be something that we center. So thank you, everybody. Feel free to make that kind of last minute sound in the in the soundboard, and then we'll end the recording. Perfect. Alright. Ciao, everybody. Thank you all so much.