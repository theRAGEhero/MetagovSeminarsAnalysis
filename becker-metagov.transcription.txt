Speaker 1: K.

Speaker 2: Thank you.

Speaker 1: Today today, we've got Josh Becker, coming out of London, to share with us, results from the collective intelligence world, with their tempting connections to all of our meta governance themes. Josh.

Speaker 2: Yeah. So so I'll go ahead and share my screen. I've got some slides, of course. I guess before I jump into it, I'll just say a little bit about my background. Prior to I was out of out of school for a while. I worked in the world, then I worked in decision facilitation. And what I've learned from that is that it's not just about who's in the room. The process made such a big difference. But, of course, I don't need to tell you all that. And I did I this is kind of for Nathan. I just I work in a business school now, but I did my PhD in communications. So I do like to hold on to that. Okay. So I'm gonna I'm gonna get started and I also just wanna acknowledge I'm feeling just a little bit under the weather today. So if I seem I'm just gonna kinda go through everything real slow and casual, but I am in fact very excited about all this stuff. Let me just get PowerPoint to play nice. Okay. Great. Okay. So I'm gonna talk today about collective intelligence. I'm gonna start pretty broadly just to give you my world view. I'm gonna go pretty quickly into focusing specifically on forecasting, and I'll I'll define what I mean by that when I get there. So switch slides here. There we go. Okay. So the basic principles of collective intelligence, a very intuitive kind of definition, is anytime the whole is greater than the sum of its parts, right? So anytime you get 10 people in a room and they do something a little bit differently than 10 people individually would do. So that leads me personally to a distinction I think is very important which is that collected intelligence is not quite the same as collective intelligence. So one of the most famous examples, thanks to James Surwicki's book on the wisdom of crowd, is this notion that if you ask a whole lot of people some question and take an average, you get collective intelligence. I would call that collected intelligence. I think the real magic happens both practically and, you know, in terms of research interest when you start looking at process. But as you know, I'm already biased towards that. So okay. Just a little bit of an idea about how I think about collective intelligence, you know, I'm thinking about teams and organizations and societies each of these different levels all have their different designs. So if something like a team might be a lot of people with similar roles within an organization more broadly, you're gonna have more clear division of labor. All of these things can end up making a difference in how we think about. Again, this is just my background world view, so I'm gonna I'm gonna skip through a lot of this stuff pretty quickly. But this this piece is important. When we talk about engineering collective intelligence, there are two things that people tends to focus on. So a lot of people focus on group composition. And when people you you you probably heard this notion that in order to get a collectively a collectively intelligent group, you need a group that's diverse. Right? Different perspectives are gonna bring different ideas or if you're, you know, making some forecasting task, my error is gonna be different than your error. So there's a lot of research on group composition. That's one important piece. But it's as I already mentioned, it's not enough to have a really great group because you can start with the most diverse group in the world. But if it's a concern issues like group think and hurting, right, you lose all that diversity through the process. K. So when I think about collective intelligence broadly, I have a few different ways I think about it. But what's really important is that depending on what task your group is doing, there's gonna be different recommendations for how to run that group. So when I was a mediator, a facilitator, right, this is conflict resolution but it's really about making a decision. One of the most important things we did as a mediator was we broke up the process into discrete chunks. In the beginning, people are trying to build understanding. They're just trying to get all their ideas and thoughts and needs and interests out on the table. At another stage, they're gonna be brainstorming solutions. What's good for sense making and building understanding is not necessarily good for brainstorming solutions. So as I get more into my deep my specific topic today, forecasting, it's just really important to keep in mind that I'm not talking about the whole world of collective intelligence. I'm focusing very narrowly on this question of forecasting. But I wanna share just a few frameworks in case you find this interesting. One of the most famous ways of dividing a group tasks is in the graph complex. They talk about generating solutions, choosing solutions, negotiating solutions, and actually executing plans. My personal model is one I've adopted from mediation. So phase one is defining the problem or sense making. Phase two is identifying solutions or brainstorming. We can actually break up phase three separately if we want into evaluating solutions and selecting solutions because we may not actually agree necessarily on preferences. That's my world view. There's another way of looking at it, which is just a little bit descriptive, like looking at it, what people do. So you can talk about problem solving, and this is often modeled as navigating a complex fitness landscape. Right? You can talk about creative generations. So this is, again, gonna line up with the brainstorming. You can talk about estimation as a specific type of task. Right? That's gonna be very specifically numeric accuracy, and that's what I'm gonna talk about today. And then you can talk about the step of idea selection. Remember, I said we can kinda break apart the evaluation and the selection. Be more of a coordination issue. Maybe we all agree on exactly what the payoff of these different solutions are, but we disagree about what our needs are or what our preferences are. Just gonna do a little coordination, a little negotiation. Okay. I'm gonna come down to numeric accuracy, and that's what I'm gonna talk about for the rest of today. Oh, by the way, I I forgot to mention when I started. The norm in my field in management is to interrupt constantly with questions. So typically at this point, in fact, people often have questions at the title slide. So, use the hand raise feature in Zoom, because that'll bring your video to the top and then I'll just give you the floor right then and there. It's a little weird for me to talk for twenty five minutes uninterrupted, but that's okay if you wanna do it that way too. Okay. So when we're talking about estimation or forecasting, we're talking about asking people questions where the answer is a number. So these could be something like we're you know, we're pushing a potential policy through congress and we wanna say, we sign this into law. How many jobs will be created, you know, by neck this time next year versus if we don't sign it into law? If we're considering buying some asset, how much money is this asset worth? Or how much water is in the moon? Just anything where you get a number at the end of the day. Okay. So the question then is if we've got a group of people, how do we get the most accurate estimates? Well, there's a really nice mathematical property that underlies the wisdom of the crowds. This is called the crowd beats averages law by Scott Page. It's also really just a reinterpretation of the standard variance bias trade off which we've all seen if if you've taken a, you know, statistical estimation theory class. So what this tells us is that the error of the average so c is that collective estimate. You just ask everybody how much water is on the moon. We just take the average. Let's see here. What this tells us is that the error of the average is equal to the average of individual error, which is not the same as the error of the average minus diversity or or variance. And what this means since the left term is equal to individual error minus diversity, collective error must necessarily be lower than average individual error. And I wanna I wanna I'm assuming since nobody's interrupting, but that makes sense. But if you're not clear on the difference between collective error and average individual error, please do ask. Yes, Zeke.

Speaker 3: I'm gonna since you invited it. So just to be clear, though, we're dealing entirely with a scenario where this is a measurement of a ground truth fact with a bunch of different sensors of it and not a circumstance where the belief of the thing affects the outcome of the thing. Right? Like Give me an

Speaker 2: example of the latter scenario so I can make sure I'm the same.

Speaker 3: Okay. So so, like, if you're talking about job creation, which was the example you made, then weirdly enough, you may be forecasting a future, but the forecast itself affects the future because it affects decisions of other actors. So you wouldn't be able to treat it the way you would create a collective sensor problem. Like, I'm all I've So if you're doing, distributed estimation theory, and you have a ground truth physical fact that you're estimating with a bunch of sensors, then, effectively, the measurement itself doesn't affect the thing being measured directly, or we can assume it doesn't. But if we're dealing with social processes where the forecast affects the behavior of other actors, which in turn affects the thing being forecasted, then I'm not sure that it makes sense to use the same, mathematical mechanism as an estimator.

Speaker 2: It might not. I think it would probably depend on the circumstance. But, probably, the solution there is to be really careful about defining the question. And this is always really important. And when I design these experiments, I spend so much time thinking about the questions. It's like you've got a genie. Right? You ask a genie. You make a wish to your genie. They're gonna give you exactly what you ask for. So you gotta be really careful about what you wish for. Right? It's gonna be similar things with these questions. So we might ask in this case, we would phrase the question something like, given the fact that we're having this conversation, you know, how many jobs did this policy create? So I think as long as there's a measurable ground truth, which there there's always gonna be some jobs created, right, then you can then this formula will definitely apply, but you have to be really careful about how you ask the question if you're concerned about these kind of feedback effects. And and and I'm thinking on the fly. Nobody's ever asked that before. So happy to discuss that further. Okay. So we have this nice mathematical property. This is what gives us the wisdom crowds. Ask a whole bunch of people a question, take the average, and the average answer is mathematically guaranteed under any circumstances. This is this is a mathematical truth. It's mathematically guaranteed to be more accurate than a randomly selected individual, assuming your question is well defined. K. So that means you're better off asking the group as a whole than one individual. Okay. So that's the kind of background of the Wisdom crowds. But then the question is, what happens when people talk to each other? Right? This is really intuitive argument that people make which is that, oh, once people talk to each other, well, they all become more similar and you lose that diversity in that right hand term. Right? But under the simplest model, if everybody just converges on the average, we have no diversity but the average hasn't changed. So it's very common for people to make intuitive arguments saying that social influence just generally undermines the wisdom of crowds and yet to keep people independent. It's not really based on anything. Intuition could be rung of sleep. And in fact, what we find in practice when I run these experiments, and I'm not the only one that's actually quite a lot of evidence supporting this if you look out there. There are pretty simple reliable conditions under which a group can positively improve through some kind of social exchange. Now social exchange is there's a lot of different ways. Social exchange can be something like the Delphi method which was something that emerged in the seventies where people are literally writing a number down on a piece of paper and like sharing the numbers or it can be just people having a conversation. And each of these processes is gonna have slightly different properties. Okay. And I'm gonna talk about individuals today, but this background is really important. So at the group level, social influence can offer benefits, but it's fragile. So the outcomes depend on things like network centralization. If you have really dominant individuals, those dominant individuals are gonna drive the group, and you're not really getting the wisdom of crowds at the end. You're getting that loudest person. And that can be so that can come through network structure, so some people are central in the network. That can come through conversations. So typically, in a conversation, some people talk more than others, and it's usually me talking a lot. So and I've I've got some experiments as a a preprint right now to review at NHB. We just put people in rooms, in a chat room online. We ask them these numeric questions, and then we say, what was the answer of the most talkative person? And we find that the most talkative person consistently predicts the outcome for the group. There's also this effect that, like, there's this belief confidence correlation. If I'm really confident, I'm gonna ignore you all, but you're still gonna be drawn towards me. So confidence can end up leading to centralization. So whether or not your confidence is correlated with accuracy can determine group outcomes. So if you have a group where the most confident people are also the most accurate, gonna pull the group towards truth. Generally, we seem to find that there's a positive correlation between confidence and accuracy. We don't seem to find that commonly thrown around Dunning Kruger effect overall, but of course, it's gonna vary widely from trial to trial or group to group. Okay. So okay. So the question then let me get my slides back. Is what's happening to individuals in the group? Everything I've said so far is focused on this average. But there are often contexts when we're gonna discuss things socially, but then go back to our own offices and make our own decisions at the end of the day. So if you talk to people who work in, like, a hedge fund or any kind of financial portfolio management, there are they are all basically kings of their own domain. Right? Like, you've got your portfolio, you're making your individual independent decisions, but you're going out with your colleagues and you're talking about market conditions. You're sharing opinions and forecasts and predictions. Right? So you're swapping information then going back at the end of the day and saying, what am I gonna do independently? And one of the really interesting things is that the group can actually get worse even as individuals get better. So I've reversed this equation here. I swapped the collective and the out average. Just to look at it from a different perspective, what this says is that average individual error now is equal to collective error minus diversity. But if we drop that diversity term term

Speaker 1: Setting it up.

Speaker 2: Yes. It should. Thank you. The point is if you drop that diversity term to zero, every at the end, average individual error equals collective error. Thank you. I'm gonna fix them. I'll do this again. So if we all converge at the end of the day and we just completely agree, then we're actually all essentially learning from the crowd. So the question is how robust is that? Can that can it even be that individuals get better while the crowd gets worse? And the answer is yes. So I'm gonna demonstrate that in three ways. I'm gonna show you that groups can get worse even as individuals in the group get better. I'm gonna do that with two theoretical models and then a reanalysis of previously published experimental data. Just gonna do quick time

Speaker 4: shift.

Speaker 2: I don't have a clock in this room.

Speaker 1: I had seven 09:25 about. Great. Alright. So till it's past about.

Speaker 2: Great. So okay. So the first theoretical model is, the opinion exchange model, which is say we're that's like that Delphi method I mentioned earlier. We're just writing our opinions down on a piece of paper and sliding them across the table. Just the numbers. So the model, this was studied by Morris de Groot in 1974, and I'm just using it here. Each individual starts with an opinion, so some numeric estimate. Each person observes the opinions of some peers, not necessarily everybody. There can be a social network going on here. And they update their belief as some kind of weighted average that combines their own opinion and peer opinions. And that's infinitely flexible. So that weighted average could place no weight on peers. Right? It could place all the weight on my own opinion and just stay stubborn. Or I could completely adopt the average of my friends or I could just adopt sex belief. Right? It's just a very generic model, and then you repeat that indefinitely. And what and de Groot showed two things. One is that the group eventually will converge on consensus, and importantly, that consensus point is the term weighted according to network centrality. Okay. So what we're doing is we're taking this model, we're simulating it just in r or or Python, and we're looking at different conditions that we expect to make groups worse, and we're asking what happened to individuals. So this is the plot of my simulated outcomes. There's no error bars because you just run it a million times. So this is basically a lazy way of getting at the, you know, mathematical structure of this model because I'm not as good at linear algebra as to group. So I'm looking at two different conditions right now. One is I'm looking at the condition where correlation between accuracy and confidence is either negative or positive. So that's the x axis all for the top plots, and and that's in a decentralized network, so everyone's equally weighted. The bottom plots hold that correlation constant at zero, and they vary the network centralization. And then we're looking at either the the crowd is initially accurate, so that's low error, or the crowd is initially inaccurate, so that's high error. So these are basically conditions, like, where on the right hand of each chart, you know, at the bottom, we the group gets worse because it's highly centralized. On the left hand at the top, the group gets worse because you have that negative confidence correlation confidence accuracy correlation. So there's a lot of points on this plot where the group gets worse. Now what happens to individuals in the group? Well, they get robustly better. And I wanna draw your attention to the right hand side, which is where we see the groups. Actually, the box is not just the right hand side. So the the right hand side, these boxes show where the individuals are getting better. So this is the regime where individuals get better and it includes some areas where the groups get worse. So what this shows is that the individuals will often, but not always get better. If that correlation is so negative so that the most confident person is super incorrect, it's gonna end up making the group worse. But we still have a wide range of areas where the group gets worse and the individuals in the group get better. And then we see with the centralization, individuals are always getting better. So highly centralized networks, lowly centralized networks, individuals generally get better. So we have a wide range of conditions. Okay.

Speaker 4: So

Speaker 2: that's model one. That's just one way of looking at things. Model two, this is kind of a custom made model, but it's just based on a general signal signal exchange model. So everybody begins with a random signal. This is something I observe about in the world, and it informs my estimate. So my signal is essentially equivalent to my estimate, but I can learn more signals. You can I can say, hey? I saw that, you know, Ukraine and Russia are having positive peace stocks. So that gives me a certain prediction about the stock market. And you can say, hey. I saw that oil prices dropped a little bit. And that gives you a certain prediction. Right? So I'm learning different pieces of information by talking to each other. My estimate's gonna be the average of the various signals. And as time progresses, we're basically picking an edge connecting two pairs. So I'm looking at z right now. We could you know, we're gonna randomly get connected, and we're gonna have a conversation and we're just gonna swap some signals that the other person doesn't have. So it's just a network where people kinda share information, share signals. What we find in this, it's a little more complicated. So I'm gonna talk you through this. So this shows the mean crowd error. The main figure shows the short term. Asymptotically, at the end of the day, if we all share all the signals, we all end up with an estimate that's equal to the average of the signals, which is the same as where we started. In the beginning, the crowd estimate was the average of all the signals because everybody had one signal and averaged their belief. At the end, the crowd estimate is the average of the signals because everybody has all the signals and their beliefs are all identical. It's nonmonotonic. In the meantime, anywhere in between those two endpoints, this information exchange is making the group average worse. Why is that happening? Because signals are becoming disproportionately represented. Let's start with the very first conversation. Before I talk to z, we all have independent signals and we're uncorrelated. Then z gives me a piece of information, I give z a piece of information, those pieces of information are now overrepresented compared to the other signals in the account. So we get this non monotonic effect, but by and large, information exchange here makes group ever worse. What happens to individuals? They get monotonically better. Long term, short term. Why does that happen? It comes back to that crowd beats averages law, but now it's like the signals. As I get more signals, my average is basically making use of more pieces of information, so I'm just gonna become more and more accurate as I hone in on what's essentially the average of all signals. So these are two theoretical models that just help us think through in a informal terms the kind of things that could be going on between what happens with the group average and what happens with individuals. So now we wanna look and yeah. Dmitry.

Speaker 4: Thank you, Joshua. So could you please make two clarifications? So first one, could you please explain what do you mean by the centralization parameter? And the second one, what do you mean by selecting an h while receiving a signal in the second model, please.

Speaker 2: Yeah. So the first question was about sorry. Could you say the first part of the question again?

Speaker 4: So what is meant under the centralization parameter? Centralization.

Speaker 2: So centralization imagine you have a network and you count up how many friends each person has. I'm not that popular, so I'm I have a low degree number of connections. Centralization basically asks how evenly distributed is that connectivity. And you can there's a few different metrics for it, but you can just use the Gini coefficients. It's a measure of inequality. A highly centralized network has a few very dominant individuals and a lot of peripheral individuals. So it would be like, if I know everybody and nobody knows each other, that's a star. You're just a star. That's the most highly centralized network possible or by certain metrics. So that's centralization. And then when I say Thank you. The US, what does it mean to select an agent? Is that right?

Speaker 4: Yes. What is meant by selecting an age here?

Speaker 2: Yes. And my apologies. I kinda was glossing over the the principles of simulation. We talk about agent based models, and agent is just a person. So it's like in you you can imagine you're basically programming like a little simulation video game. So select an agent just means select a person. Does that help?

Speaker 4: Sorry for my mispronunciation. I was asking about the point number three. What do you mean by edge selection?

Speaker 2: Oh, an edge. So you have a network is made of two things, nodes and edges. So it just means select an edge, which is to say two people. Just select a pair of people that know each other to have a conversation.

Speaker 4: Okay. Thank you. Thank you so much for dedicating your time.

Speaker 3: Yeah. I was gonna ask some quite clarifying clarification questions regarding the the graph model. Like, is it directed? Is it weighted? What's the graph generating process? Normally, I would expect something like preferential attachment for this kind of social system.

Speaker 2: Yeah. I just don't remember exactly what algorithm we use. It's generally gonna be a prep so I am studying fairly simple networks and just varying centralization. It's gonna be probably a Barabasi Albert network. I think I actually implemented this in r. So, yeah, that's a preferential attachment.

Speaker 3: That's preferential attachment. Yeah. But also the question of, I guess, it's probably unweighted and undirected. I just to your point earlier, the topology will have a effect on the quality of the the estimation process. And so, you know, if you're using a singular dimension like centralization, then that the notion is predicated on, you know, again, this whatever the underlying graph model is and then your reduction to that centralization measurement in the family of graphs generated by that graph generating process. So I was just trying to quickly get my head around what, you know, what the base assumptions are underneath these results.

Speaker 2: Yeah. Absolutely. I'm I'm super simple stuff. Basically, random preferential attachment, nothing fancy, undirected, unweighted. But my intuition is that if we were to go study other types of networks and we were to make it directed, if we were to look at different ways of generating but measuring centralization, we get these same results. And I went back a couple slides just to remind you, the only thing that we found that could make individuals worse was this strong correlation between confidence and error in this one case where the group is initially inaccurate. So that's like the one case that makes individuals worse. For the centralization, we find that individuals get better across the spectrum. So I'd be surprised if there were different network structures that undermine

Speaker 3: Usually, the usually, the analytic results for something like this will be related to the the Basically, the eigenvalues of the matrix representation of the graph. So insofar as the estimation properties do have an analytic relationship to graph topology, it's usually something that is expressed by that, the, like, the the that analytic description, the the I the relate basically, the eigenstructure of the matrix related to the graph.

Speaker 2: That's de Groot's result. De Groot's result is for undirected weighted networks, and he shows exactly that. It's a super generic mathematical result. Groups converge on an eigenvector weighted.

Speaker 3: It it's pretty standard. It's like, like, if you're doing, like, spectral graph theory with respect if you're doing estimation on, like, network problems, then there's a different results under different assumptions. So it sounds like the one that you've cited is related to these undirected graphs. But there's also time vary there are results on time varying graphs. There are results on sorry. For for context, I did distributed estimation and decision making for my PhD work, so I'm a little bit overkilling this. But, point is, I think it's important to know that the underlying graph model in the sort of computational social science setting may have a lot of bearing on the the eigenstructure of the graph that you've generated. And so then your numerical results will be generally expressing, something that is also likely an analytic property of the family of graphs that you use to generate the simulations.

Speaker 2: Oh, I mean, it must be an analytic property. Right? Our our simulations can only show, you know, what the group proves, some some various iteration of that. I wanna ask you I wanna ask you a question. See, what would you say is the kind of practical takeaway of that point? Just so I can make sure

Speaker 3: My my main practical takeaway is that, often we don't know what the topological structures of the so the, the social systems we're studying are. And, as a rule, I generally try multiple different graph models and look at how resilient my results are to swapping out the, basically the generating processes. So if your results only hold for a particularly, like, small subset of graph generating processes, then I tend to not keep them. But if I could take something like yours with the, let's say, the unweighted, undirected graphs, I might sort of swap in some weighted graphs. I might swap in directed graphs. I might try a different graph generating process. And then I would walk away feeling like my computational results were a lot more robust if I could sort of make conclusions that spanned or sort of were very strong in the variation across, basically models of the social processes because we know that we don't know that much about the real social processes.

Speaker 2: Yeah. No. That's true. I cannot I'm gonna assert without, evidence that I think that this will be a very robust computational result on the on the bottom half with network structure. We can see the correlation is already a little shaky. Just because what this represents on the x axis here is basically a range from a random Poisson graph to a star graph. And going on to Groot's results, the only thing that really matters is a person's centrality. But perhaps there's some edge case that I've missed. Okay. So the last thing I did oh, and actually the the the reason is a really important point here, which is that we're at least what I'm thinking about. I'm talking about process design. I'm talking about engineering. So I'm far less concerned about, you know, so called natural network emergence structures, the kind of things people are measuring with, like, irghams and stuff. Because I'm saying, I've got a group of people. How can I structure them as an organization? Although, I suppose, at the end, you're gonna say practically, you don't have to worry about it. So yeah. The remote people don't

Speaker 3: adhere to the structure you impose on them, though. They tend to they they you give them some structure, and they follow it, and then they the extra links appear naturally. So I don't know that I've ever experienced them in a management setting. The

Speaker 2: I mean, if you're if you're I mean, digital technology, we we should maybe pick this piece up later, but digital technology, I think if you're trying to build a collective intelligence platform and say, I wanna solicit a whole bunch of opinions, then I'm gonna kinda let people talk to each other. Digital technology gives you a lot of control over who talks to who and and things like that. But your question is actually really important, I suppose, for the hedge fund type thing. So my claim right now is gonna be it still works for the hedge fund no matter what the network structure is, but I haven't actually calculated that. Yeah. Seth?

Speaker 1: Oh, I just wanted to make sure that you've got time we were I have are we into questions now, or did you wanna do us including slides before we dive in discussion proper?

Speaker 2: We I'm gonna say both. We're into questions and I have one last thing to show you. Basically so I'll I'll push ahead. It sounds like you wanna get into discussion. I went and I got four publicly available experimental datasets that had control. And the basic paradigm here is that people make an estimate about some kind of data value. They talk or exchange opinions. I shouldn't have used the word talk there. And then they have a they make another estimate. So estimate, some kind of social exchange, another estimate. The number two here has people actually having a conversation as does number no. And then number one, three, and four are just number exchange. Basically, what we find is that for each of these types of exchange, so I've got a centralized opinion exchange is the blue bar here. I've got decentralized opinion exchange that is a sparse network where everybody's got four peers. These are groups of 40. I've got people in a chat room, and I've got the control group with no social signal. In every condition, the average individual is getting better. That's across the board. Now we can go in and say, what if we divide the trials up into ones where the group got worse and ones where the group got better? So now I'm actually conditioning on the outcome variable. I'm not conditioning on like the in input variable, but I just wanted to see what happens when the group get worse. Even when the group gets worse, individuals are nominally improving. So by nominally improving, I'm looking here. I'm saying they're they're negative values but they're not statistically significant from zero. So we can say that even when the group gets worse, individuals are not getting worse, they're probably getting better. Okay. That is and then I broke it down in a bunch of details. You can ask about error quartiles later. So that's it. Implications are that social learning is very robust for individuals. So although now I'll acknowledge there may be some edge cases but I don't think so. We don't need to worry too much about process. Have people talk to each other. Have them share information. They're basically learning from the wisdom of crowds. And I have one caveat that I don't have time to get into today. Something I call the crowd classification problem. You have to ask people numbers. So if you're asking, how much money are we gonna make from this venture? You're good. Nice and robust. People can learn from each other. If you're asking, will this turn a profit? Yes or no? It all falls apart. The lesson there is that nuance matters. Okay. I'll close with that. Thanks so much.

Speaker 1: Fantastic, Josh. Thanks so much. I so I I was really eager to have you here for the reason that these are slightly different communities, the collective intelligence and the governance. I wasn't able to quite put my my finger on it until Divya's question, which is really questioning ground truth. We we have these methods for, when there is a truth, coming to decision, that's kinda your domain, and governance seems to be, coming to decision when there's not one. And so I'm curious about that bridge. Maybe, I should just not trample too much on Divya's question, but I do see, certainly my interest in this discussion is what that bridge looks like. Divya, can you, represent your own question?

Speaker 2: So I'm just looking at the chat here. I didn't I didn't see Divya's question, but I think sorry. Did you want me to respond, or did you want Divya to share the question?

Speaker 1: Divya, take it away. Are you there, Divya?

Speaker 5: Yeah. I had just asked about and I think this may be quite a basic question around, like, forecasting versus the kinds of decisions that come up in governance, which are often more about, like, values adjudication or, you know, best path forward type questions as opposed to trying to get to a ground truth, and how much of these findings would apply in those cases.

Speaker 2: It's it's like a nonsensical to ask how accurate am I when I say I prefer warm weather to cold weather. So if you're talking about preferences, then none of this has any meaning, except to the extent that the the group model suggests, you know, people. So assuming people converge, that influential people will be more influential, basically. But what I would do is challenge you to find the measurable questions. And and it's it's really hard to say this in the abstract that you can ask questions like, what is people's preference? Right? So so when you're got when there's governance going on, you're speaking to the values committee presumably, but also you're concerned about the values of the organization as the whole and what people prefer. So you can talk about ground truth if you're trying to if there's some uncertainty about what people want. But, yeah, there's just no notion about preferences. And do you have a specific example oh, go ahead.

Speaker 3: I was just gonna say, I think, at least for me, the problems are much more closely related because, you can frame social choice problems as estimation problems as well. They just to Divya's point, they they don't have, ground truth. You have you have select you have available options that you select from. You have utility functions for the individual members of the network. You can do the same kind of network mixing process style analysis, but instead, each individual has a a preference function over the selection of possible choices and that the collective decision making apparatus has to come to a decision, which is like a forecast in that it's one value despite the fact that every individual would provide a separate one, And that in the application of, estimation theory to collective decision making, you run into this immediate problem of things like averages being really problematic, and you get things like Rawls' proposal for, using min max optimization instead of using averages as a way of measuring the collective utility precisely because it generates a different, like, I guess, lived experience for the participants in that system. But it still very much leverages, or it can be framed in the way that you framed the forecasting problem as long as you frame it as a social choice problem and use the same family of machinery.

Speaker 2: I wanna push back on that just to to get you to clarify. So I'll agree that I mean, the the group model is a group model of opinion exchange. It's not a a model of forecasting exchange. But the whole framing of this problem is how do we make more groups more accurate. So there's no notion of error. If there's no notion of accuracy, then none of this has any bearing. But as soon as you start talking about utility, now there's a notion of accuracy. Right? Maybe we can't actually go out and measure it, but there's a conceptual correctness in the sense that one decision will lead to higher utility than the other decision. We may never know what the answer is, but utility is something that's presumably object, if not.

Speaker 3: Sure. Well, it's localized when you when you place the individual utility or preference function on the individual agent, and then you have the aggregation problem where you're trying to produce, again, what is the social utility as an aggregation over the individual utilities, which gets you the differentiation between, say, summing versus min max. But when you're talking about the forecasting problem, if you wanna frame them on the same plane, then what you're basically saying is that accuracy is what everyone wants. And so the idea that the the social utility is the accuracy of the estimate is what puts them on the same plane.

Speaker 2: Exactly. So if your question to the crowd is we have some decision we need to make, what will the social utility of this decision be, and we define social utility as whatever. Now you have something with a a ground truth. Ground truth isn't always measurable. Right? Right now, we have no idea how much water is there there is on the moon, but there is a correct answer.

Speaker 3: But I think the key here is that the breakdown between a a governance problem and an s and a forecasting problem is that in one case, you can assume that the ground truth exists even if you can't measure it. In the other case, you actually are moving in another direction where the the notion of the of a ground truth correct answer, like, may not even exist. It sort of emerges from the process the decision emerges from the process itself. And to your point, I can't necessarily measure it against the ground truth optimal because there is none. And so that's where the I think, actually, a lot of the really interesting stuff comes up is when you look at what can you and can't you do in a circumstance where not only do you not know the, quote, unquote, ground truth, but actually it doesn't exist and that the process is effectively producing it or or or it's manifesting an outcome that can have a measured sort of you can measure how people feel about it, say, but you can't necessarily say that it was optimal in the same way that you could for the accuracy of a forecast.

Speaker 2: Yeah. I think it's hard again, it's hard to talk about this in the abstract and we're getting into some philosophical blurriness, but if you're talking about utility question function and the question is what would the social utility of this be, Then all of this applies because there is a ground truth. Are we agreeing on that, or are we disagreeing?

Speaker 3: No. I'm saying that the social utility function, the aggregation, even if each individual actor in the network has their own utility function, that there's a there's a formal fuzziness in the sense that there's no unique aggregator. So if you compare the aggregation of, like, a min max versus an average, you will get different, quote, unquote, optimal results, which means there is no optimal result because the framing of the problem is what determines what is optimal, unlike in the forecasting problem where effectively accuracy we can pretty agree that the the the social utility is it's accurate. Whereas in the so our governance problem, we would have to all agree on precisely a aggregation across the individual utilities, and thus there cannot exist a unique global optimal because we could still disagree on the thing that takes the individual utilities to a global utility. The basically, min

Speaker 2: max versus that. To the g Yeah. It comes back to the genie. You get what you wish for. So if so first, you have to define social utility and you have to say so the question is, like, what's the social utility of this policy if we define social utility as the average or as the min box. Yes. And and I don't think that always necessarily makes sense. Right? Like, it's not always desirable to convert every question into, like, a quantitative objective kind of framework. I also teach negotiation, you know, this whole other world of, like, there is no correct answer. There's just what I want and what you want, and we're try and that's a lot of what governance is. So I would say the way I think about this and this question is, you don't go it's not like you necessarily go looking for opportunities to make something objective and measurable. Although I'll admit that's kind of my personal style in the world. Actually, my first job no. My second job out of college, I worked for a customer service department as the manager, and I made a and we had this rubric where we score phone calls. So my hiring I hired people. So my, like, the only question I had to ask when I hired someone is, what score are you gonna get? That's, like, all you know, that that so that's how I tend to frame things. It's gonna be context dependent on on how appropriate that is. But the point is, when you find yourself in a situation where you're like, I need an accurate answer to this question, that's when we start thinking, okay, how do I take advantage of collective intelligence or the wisdom of crowds in some way? Because I tried to resist the the notion of what is it when you have a hammer, everything looks like a nail. So I tried to resist that and I tried to say, look, I'm good at hammering, but until I see a nail, I'm I'm not gonna worry about it.

Speaker 1: I think we have a question from Aviv, and then we can do go back to Divya.

Speaker 6: Sure. One second. So I guess and I and I apologize. I did miss a bunch. I had a meeting run late. So but I I this has been a really fascinating conversation. And I guess, like, this, like, sort of summarize what I'm what I'm understanding is I think, like, the way to like, the way I expressed it was is this about defining the difference between a like, given an objective function that involves rating items, this is useful. Otherwise, it's not. But it it it it it it's like but you can't it's not at all about deciding which objective function you have unless you, again, have some other objective function that that is being used to decide what that objective like, basically, like, it like it it like, that now that's my understanding from this is you need to have some basis of an objective function, and then you can use this to rate items. And I I give the example that I think about, which is a governance policy for addressing this information on Facebook. At what level of abstraction? Like, is this sort of thing useful? Maybe you can use it to rate the applicability of policies. Like, groups can say, oh, this policy is useful for achieving this particular goal around misinformation on Facebook. But whether or not we we wanna do that at all, it's not gonna help you figure that out. Is that right?

Speaker 2: I agree. I I think so. I I guess the the comment about Facebook, that went by really fast, but I think so. It because not everything in the world is about accuracy. Is that what you're getting at?

Speaker 6: Or is that I think it's more it's more there's a diff there's just a difference between, like, determining what the objective function is eliciting, what our objective object what our objective function should be is different from saying, oh, how well does this thing get toward that objective function? How good is it at doing that thing? And and I think that is maybe the difference here. Like, if you're saying which of these policies will will minimize the amount of of information that make people unhappy that they've read it later because they felt misled. Sure. That's an that's a well defined objective function in some setting up. Super well defined. But you can then have people rate a set of policies on that. But if you're like, what should we do on misinformation on Facebook? Like, that is not this is totally useless for that.

Speaker 2: Right. You just should is a moral, ethical question. Right. Exactly. Just wanted to bring the slide back up to kinda put this back in the broader context of collective intelligence, which is that there are different phases of decision making and each phase requires different things. So what you're talking about is like the sense making defining the problem. If we're negotiating what is our objective function, that is sense making. Only once we objective function ideas evaluation is gonna come in after you've after you sorted all this stuff out.

Speaker 6: Yeah. That sound that that makes sense.

Speaker 4: Good evening.

Speaker 1: Go ahead, Damian.

Speaker 5: I I mean, I'd love to hear more about what either you were so you put in the chat as a response to the more of just a comment around the point of governance and what stage incorporates the most, I guess, decision making capacity.

Speaker 2: What stage incorporates the most decision making capacity? And I wanna acknowledge I'm not even I haven't been tracking everything in the chat, but I don't understand.

Speaker 5: Like, I guess, at at what point you know, there's kind of a who decides what is decided question in governance. There's a who decides who decides what is decided question in governance, and then there is kind of the what is decided preference aggregation part. And so I I guess we were just thinking about which piece of those things is most relevant here, can most be put into the forecasting frame, is my understanding.

Speaker 2: Yeah. You know, I I think that sounds like it would be consistent with my thinking of it as these, like, different stages and essentially what you're talking about is just a completely different part of the process. It's a different part of the process than I than I've been looking at. I think so. You evoked it. Which says it's not about accuracy.

Speaker 3: So you evoked it in your problem when you talked when your first slide or early slide when you talked about problem framing, the sense making part, I think what's happening is that at least in the governance context, the that becomes such a point of leverage because when you define the problem you're solving, you're sort of you're affecting what will even be considered a good decision. And if you if you, for example, were to frame a forecasting problem as a way of in in supporting a decision making problem, like, will if we if you you pose the question about the how many new jobs will be created if we pass this proposal, that is actually a way of approaching, should we pass this proposal? And, like, what ends up happening then is that the act of framing the problem as how many new jobs will be created if we, pass this proposal had a huge bearing on the decision. Even if the forecast is right, you may or may not have actually succeeded in driving the desired outcome associated with the many constituents in that system.

Speaker 2: Yeah. I mean, depending on what your desired outcome, I think maybe the flip side of it is that people use those points of defining the problem as opportunities to get what they really want. I mean, this happens all the time in in, like, US governance. Like, you look at, like, why did this happen? It's because, oh, someone defined the metric this way. You know, I'm thinking of the scene in the TV show West Wing where they realized that the whole measurement of, like, our people in poverty was made up by some, like, lady sitting in a desk in, like, the seventies. And that somehow because that measurement got in there, you know, all the policy that came after was

Speaker 3: Just to be clear, that's why I did my big dispersion earlier, like, the the exploration of the bottling assumptions. Because at least in my experience, the counter to that sort of framing the problem as being the main point of power is that to set a norm of and, again, this is in community maybe governance, not in in forecasting. But by, by breaking the norm of a single unique model of the world, you get a more expressive understanding of the decision making landscape. And so, you know, this practically speaking, you make assumptions, you design metrics, whatever, you preserve the plurality in the representation of the problem, including things like metrics and process models, then you do your analysis over many. You provide a basis of information through which people can form fact factually informed opinions, but, like, not necessarily presuming everyone agrees

Speaker 2: on the

Speaker 3: most important metric or the most important sort of you know what I mean?

Speaker 2: One of the things. We're Yeah. We're get we're getting on

Speaker 1: the hour. So I see your hands up.

Speaker 2: Do you have

Speaker 1: a general question, or did you have a question?

Speaker 6: I I had a question.

Speaker 2: I I wanna make a comment for you.

Speaker 1: I'm actually going okay. Josh, and then I'm gonna close it out for the hour.

Speaker 2: Just to point out that I see this debate in here about which is better, min, max, or average. So what you could do as a group is you could forecast both and then let people use that in their decision making. Right? You're never gonna make a decision on one number anyway. Okay. Sorry. Go ahead.

Speaker 1: Great. So, yeah, I like I I wanna not push us over. So, Josh, we have a little tradition where everybody unmutes on 33 so you can get an actual real applause. So one, two really interesting. 3. Everybody unmute. I'm gonna try it again. I didn't see anybody on mute. +1, 23. Okay. There it is.

Speaker 2: And I'm applauding you all because this is really interesting. These are great questions. Thank you so much.

Speaker 1: That's all I'm gonna give everybody.

Speaker 2: Am I allowed to stick around and chat with Scent, or do we need do we

Speaker 1: need to Absolutely. Yeah. Yeah. Yeah. Yeah. Okay. Yeah. Do you have anyone who wants to stick around is welcome too.

Speaker 2: K. And I I should go in a minute. It's it's about five here, and I'm getting a little